#!/usr/bin/env python3
"""
éªŒè¯çœŸå®ç©ºé—´è½¬å½•ç»„æ•°æ®é›†çš„å®Œæ•´æ€§å’Œå¯ç”¨æ€§
ç¡®ä¿æ‰€æœ‰æ•°æ®é›†éƒ½ç¬¦åˆæµ‹è¯•è¦æ±‚
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
import warnings
import json
import traceback
from typing import Dict, List, Optional, Tuple

warnings.filterwarnings('ignore')

# è®¾ç½®æ•°æ®ç›®å½•
REAL_DATASETS_DIR = Path(__file__).parent / 'datasets' / 'real_datasets'

def validate_single_dataset(h5ad_file: Path) -> Dict:
    """éªŒè¯å•ä¸ªæ•°æ®é›†"""
    import scanpy as sc
    
    validation_result = {
        'file': h5ad_file.name,
        'file_path': str(h5ad_file),
        'valid': False,
        'errors': [],
        'warnings': [],
        'metadata': {}
    }
    
    try:
        # 1. æ–‡ä»¶è¯»å–æµ‹è¯•
        adata = sc.read_h5ad(h5ad_file)
        validation_result['metadata']['readable'] = True
        
        # 2. åŸºæœ¬ç»“æ„éªŒè¯
        validation_result['metadata']['n_cells'] = adata.n_obs
        validation_result['metadata']['n_genes'] = adata.n_vars
        validation_result['metadata']['file_size_mb'] = round(h5ad_file.stat().st_size / 1024 / 1024, 2)
        
        # 3. æ•°æ®ç±»å‹éªŒè¯
        if hasattr(adata.X, 'dtype'):
            validation_result['metadata']['X_dtype'] = str(adata.X.dtype)
        
        # 4. ç©ºé—´åæ ‡éªŒè¯
        has_spatial = 'spatial' in adata.obsm
        validation_result['metadata']['has_spatial'] = has_spatial
        
        if has_spatial:
            spatial_coords = adata.obsm['spatial']
            validation_result['metadata']['spatial_shape'] = list(spatial_coords.shape)
            validation_result['metadata']['spatial_dims'] = spatial_coords.shape[1]
            
            # æ£€æŸ¥ç©ºé—´åæ ‡çš„æœ‰æ•ˆæ€§
            if np.any(np.isnan(spatial_coords)):
                validation_result['warnings'].append('ç©ºé—´åæ ‡åŒ…å«NaNå€¼')
            
            if np.all(spatial_coords == 0):
                validation_result['warnings'].append('æ‰€æœ‰ç©ºé—´åæ ‡éƒ½ä¸º0')
            
            # æ£€æŸ¥åæ ‡èŒƒå›´
            coord_ranges = {
                'x_min': float(spatial_coords[:, 0].min()),
                'x_max': float(spatial_coords[:, 0].max()),
                'y_min': float(spatial_coords[:, 1].min()),
                'y_max': float(spatial_coords[:, 1].max()),
            }
            validation_result['metadata']['coordinate_ranges'] = coord_ranges
        else:
            validation_result['warnings'].append('ç¼ºå°‘ç©ºé—´åæ ‡ä¿¡æ¯')
        
        # 5. è¡¨è¾¾æ•°æ®éªŒè¯
        if adata.X.shape[0] == 0 or adata.X.shape[1] == 0:
            validation_result['errors'].append('è¡¨è¾¾çŸ©é˜µä¸ºç©º')
        
        # æ£€æŸ¥ç¨€ç–åº¦
        if hasattr(adata.X, 'todense'):
            # ç¨€ç–çŸ©é˜µ
            sparsity = 1 - adata.X.nnz / (adata.X.shape[0] * adata.X.shape[1])
            validation_result['metadata']['sparsity'] = round(sparsity, 4)
            validation_result['metadata']['matrix_type'] = 'sparse'
        else:
            # å¯†é›†çŸ©é˜µ
            sparsity = (adata.X == 0).sum() / adata.X.size
            validation_result['metadata']['sparsity'] = round(float(sparsity), 4)
            validation_result['metadata']['matrix_type'] = 'dense'
        
        # 6. å…ƒæ•°æ®éªŒè¯
        obs_columns = list(adata.obs.columns)
        var_columns = list(adata.var.columns)
        validation_result['metadata']['obs_columns'] = obs_columns
        validation_result['metadata']['var_columns'] = var_columns
        validation_result['metadata']['n_obs_columns'] = len(obs_columns)
        validation_result['metadata']['n_var_columns'] = len(var_columns)
        
        # 7. obsmå’Œvarmæ£€æŸ¥
        obsm_keys = list(adata.obsm.keys())
        varm_keys = list(adata.varm.keys())
        validation_result['metadata']['obsm_keys'] = obsm_keys
        validation_result['metadata']['varm_keys'] = varm_keys
        
        # 8. unsæ£€æŸ¥
        uns_keys = list(adata.uns.keys())
        validation_result['metadata']['uns_keys'] = uns_keys
        
        # 9. æ•°æ®è´¨é‡æ£€æŸ¥
        if adata.n_obs < 10:
            validation_result['warnings'].append(f'ç»†èƒæ•°é‡è¿‡å°‘: {adata.n_obs}')
        
        if adata.n_vars < 10:
            validation_result['warnings'].append(f'åŸºå› æ•°é‡è¿‡å°‘: {adata.n_vars}')
        
        if validation_result['metadata']['sparsity'] > 0.99:
            validation_result['warnings'].append(f'æ•°æ®æåº¦ç¨€ç–: {validation_result["metadata"]["sparsity"]:.3f}')
        
        # å¦‚æœæ²¡æœ‰ä¸¥é‡é”™è¯¯ï¼Œæ ‡è®°ä¸ºæœ‰æ•ˆ
        if not validation_result['errors']:
            validation_result['valid'] = True
            
    except Exception as e:
        validation_result['errors'].append(f'è¯»å–å¤±è´¥: {str(e)}')
        validation_result['metadata']['exception'] = str(e)
    
    return validation_result

def validate_all_datasets() -> Dict:
    """éªŒè¯æ‰€æœ‰æ•°æ®é›†"""
    print("å¼€å§‹éªŒè¯çœŸå®ç©ºé—´è½¬å½•ç»„æ•°æ®é›†...")
    print(f"æ•°æ®ç›®å½•: {REAL_DATASETS_DIR}")
    
    # æŸ¥æ‰¾æ‰€æœ‰h5adæ–‡ä»¶
    h5ad_files = list(REAL_DATASETS_DIR.rglob('*.h5ad'))
    h5ad_files.sort()
    
    print(f"æ‰¾åˆ° {len(h5ad_files)} ä¸ªæ•°æ®é›†æ–‡ä»¶")
    
    validation_results = {
        'total_files': len(h5ad_files),
        'valid_files': 0,
        'invalid_files': 0,
        'files_with_warnings': 0,
        'total_cells': 0,
        'total_genes_unique': set(),
        'total_size_mb': 0,
        'validation_timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
        'individual_results': []
    }
    
    # éªŒè¯æ¯ä¸ªæ–‡ä»¶
    for i, h5ad_file in enumerate(h5ad_files, 1):
        print(f"\n[{i:2d}/{len(h5ad_files)}] éªŒè¯ {h5ad_file.name}...")
        
        result = validate_single_dataset(h5ad_file)
        validation_results['individual_results'].append(result)
        
        # æ›´æ–°ç»Ÿè®¡
        if result['valid']:
            validation_results['valid_files'] += 1
            print(f"    âœ“ æœ‰æ•ˆ - {result['metadata'].get('n_cells', 0)} cells, {result['metadata'].get('n_genes', 0)} genes")
            
            # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
            validation_results['total_cells'] += result['metadata'].get('n_cells', 0)
            validation_results['total_size_mb'] += result['metadata'].get('file_size_mb', 0)
            
        else:
            validation_results['invalid_files'] += 1
            print(f"    âŒ æ— æ•ˆ")
            
        # æ˜¾ç¤ºé”™è¯¯å’Œè­¦å‘Š
        for error in result['errors']:
            print(f"    ğŸš« é”™è¯¯: {error}")
            
        for warning in result['warnings']:
            print(f"    âš ï¸  è­¦å‘Š: {warning}")
            
        if result['warnings']:
            validation_results['files_with_warnings'] += 1
    
    return validation_results

def generate_validation_report(validation_results: Dict) -> None:
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    report_file = REAL_DATASETS_DIR / 'validation_report.json'
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(validation_results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\n=== éªŒè¯æŠ¥å‘Š ===")
    print(f"æŠ¥å‘Šä¿å­˜è‡³: {report_file}")
    print(f"éªŒè¯æ—¶é—´: {validation_results['validation_timestamp']}")
    print(f"æ€»æ–‡ä»¶æ•°: {validation_results['total_files']}")
    print(f"æœ‰æ•ˆæ–‡ä»¶: {validation_results['valid_files']}")
    print(f"æ— æ•ˆæ–‡ä»¶: {validation_results['invalid_files']}")
    print(f"æœ‰è­¦å‘Šæ–‡ä»¶: {validation_results['files_with_warnings']}")
    print(f"æ€»ç»†èƒæ•°: {validation_results['total_cells']:,}")
    print(f"æ€»æ•°æ®å¤§å°: {validation_results['total_size_mb']:.1f} MB")
    
    # ç”Ÿæˆç®€åŒ–æ‘˜è¦
    summary_data = []
    for result in validation_results['individual_results']:
        if result['valid']:
            summary_data.append({
                'dataset': result['file'],
                'status': 'âœ“ æœ‰æ•ˆ',
                'n_cells': result['metadata'].get('n_cells', 'N/A'),
                'n_genes': result['metadata'].get('n_genes', 'N/A'),
                'has_spatial': result['metadata'].get('has_spatial', False),
                'sparsity': result['metadata'].get('sparsity', 'N/A'),
                'file_size_mb': result['metadata'].get('file_size_mb', 'N/A'),
                'warnings': len(result['warnings']),
                'errors': len(result['errors'])
            })
        else:
            summary_data.append({
                'dataset': result['file'],
                'status': 'âŒ æ— æ•ˆ',
                'n_cells': 'N/A',
                'n_genes': 'N/A',
                'has_spatial': False,
                'sparsity': 'N/A',
                'file_size_mb': result['metadata'].get('file_size_mb', 'N/A'),
                'warnings': len(result['warnings']),
                'errors': len(result['errors'])
            })
    
    summary_df = pd.DataFrame(summary_data)
    summary_csv = REAL_DATASETS_DIR / 'validation_summary.csv'
    summary_df.to_csv(summary_csv, index=False)
    print(f"éªŒè¯æ‘˜è¦ä¿å­˜è‡³: {summary_csv}")
    
    # æ˜¾ç¤ºé—®é¢˜æ–‡ä»¶
    problem_files = [r for r in validation_results['individual_results'] 
                    if not r['valid'] or r['warnings']]
    
    if problem_files:
        print(f"\n=== éœ€è¦å…³æ³¨çš„æ–‡ä»¶ ===")
        for result in problem_files:
            print(f"\n{result['file']}:")
            for error in result['errors']:
                print(f"  ğŸš« {error}")
            for warning in result['warnings']:
                print(f"  âš ï¸  {warning}")
    else:
        print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®é›†éƒ½é€šè¿‡éªŒè¯ï¼")

def main():
    """ä¸»å‡½æ•°"""
    try:
        import scanpy as sc
        print(f"âœ“ scanpyç‰ˆæœ¬: {sc.__version__}")
    except ImportError:
        print("âŒ scanpyæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡ŒéªŒè¯")
        print("è¯·å®‰è£…: pip install scanpy")
        return
    
    if not REAL_DATASETS_DIR.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {REAL_DATASETS_DIR}")
        return
    
    try:
        # æ‰§è¡ŒéªŒè¯
        validation_results = validate_all_datasets()
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_validation_report(validation_results)
        
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­éªŒè¯")
    except Exception as e:
        print(f"\nâŒ éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()