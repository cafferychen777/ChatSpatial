# ChatSpatial Core Tools CI/CD Integration Test Configuration
# GitHub Actions workflow for automated integration testing

name: ChatSpatial Core Integration Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'chatspatial/**'
      - 'tests/**'
      - 'requirements*.txt'
      - 'pyproject.toml'
  pull_request:
    branches: [ main ]
    paths:
      - 'chatspatial/**'
      - 'tests/**'
      - 'requirements*.txt'
      - 'pyproject.toml'

jobs:
  integration-test:
    name: Core Tools Integration Test
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
            
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libhdf5-dev pkg-config
          
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install psutil matplotlib
          
      - name: Verify installation
        run: |
          python -c "import scanpy; import pandas; import numpy; print('âœ… Core dependencies installed')"
          python -c "import chatspatial; print('âœ… ChatSpatial package importable')"
          
      - name: Run integration tests
        id: integration-test
        run: |
          cd tests/comprehensive_testing_2024/core_tools_tests
          python quick_integration_test.py
          
      - name: Parse test results
        if: always()
        run: |
          cd tests/comprehensive_testing_2024/core_tools_tests
          python -c "
          import json
          import glob
          import sys
          
          # Find the latest results file
          result_files = glob.glob('quick_test_results_*.json')
          if not result_files:
              print('âŒ No test results found')
              sys.exit(1)
          
          latest_file = max(result_files)
          print(f'ðŸ“„ Reading results from: {latest_file}')
          
          with open(latest_file, 'r') as f:
              results = json.load(f)
          
          # Check success
          if not results.get('success', False):
              print(f'âŒ Integration test failed: {results.get(\"error\", \"Unknown error\")}')
              sys.exit(1)
          
          # Check performance
          total_time = results.get('total_time', 0)
          benchmark_time = 30.0  # 30 second benchmark
          
          if total_time > benchmark_time:
              print(f'âš ï¸ Performance warning: {total_time:.2f}s > {benchmark_time}s benchmark')
              # Don't fail on performance warning in CI
          
          print(f'âœ… Integration test passed in {total_time:.2f}s')
          
          # Print step breakdown
          for step in results.get('steps', []):
              name = step.get('name', 'unknown')
              time = step.get('time', 0)
              success = 'âœ…' if step.get('success') else 'âŒ'
              print(f'  {success} {name}: {time:.2f}s')
          "
          
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: integration-test-results-${{ matrix.python-version }}
          path: tests/comprehensive_testing_2024/core_tools_tests/quick_test_results_*.json
          retention-days: 30
          
      - name: Performance regression check
        if: github.event_name == 'pull_request'
        run: |
          cd tests/comprehensive_testing_2024/core_tools_tests
          python -c "
          import json
          import glob
          
          # Find latest results
          result_files = glob.glob('quick_test_results_*.json')
          latest_file = max(result_files)
          
          with open(latest_file, 'r') as f:
              results = json.load(f)
          
          total_time = results.get('total_time', 0)
          
          # Performance thresholds
          WARN_THRESHOLD = 15.0   # Warn if > 15s
          FAIL_THRESHOLD = 30.0   # Fail if > 30s
          
          if total_time > FAIL_THRESHOLD:
              print(f'âŒ PERFORMANCE REGRESSION: {total_time:.2f}s > {FAIL_THRESHOLD}s threshold')
              exit(1)
          elif total_time > WARN_THRESHOLD:
              print(f'âš ï¸ Performance warning: {total_time:.2f}s > {WARN_THRESHOLD}s (threshold: {FAIL_THRESHOLD}s)')
          else:
              print(f'âœ… Performance good: {total_time:.2f}s < {WARN_THRESHOLD}s')
          "

  # Optional: Extended integration test (runs on schedule)
  extended-integration:
    name: Extended Integration Test
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || (github.event_name == 'push' && github.ref == 'refs/heads/main')
    timeout-minutes: 60
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.10
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install psutil matplotlib seaborn
          
      - name: Run extended integration tests
        run: |
          cd tests/comprehensive_testing_2024/core_tools_tests
          # Run with multiple datasets (if they exist and system has enough memory)
          timeout 3600s python test_integration_performance.py || echo "Extended test completed with timeout/memory limits"
          
      - name: Generate performance report
        if: always()
        run: |
          cd tests/comprehensive_testing_2024/core_tools_tests
          # Generate report from any results that were created
          python -c "
          import glob
          result_files = glob.glob('integration_test_results_*.json')
          if result_files:
              latest_file = max(result_files)
              print(f'Generating report from: {latest_file}')
              exec(open('generate_performance_report.py').read())
          else:
              print('No extended test results to process')
          "
          
      - name: Upload extended results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: extended-integration-results
          path: |
            tests/comprehensive_testing_2024/core_tools_tests/integration_test_results_*.json
            tests/comprehensive_testing_2024/core_tools_tests/integration_performance_report_*.md
            tests/comprehensive_testing_2024/core_tools_tests/performance_charts_*.png
          retention-days: 90

# Scheduled run for performance monitoring
  scheduled-performance-check:
    name: Weekly Performance Baseline
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Performance baseline check
        run: |
          cd tests/comprehensive_testing_2024/core_tools_tests
          python quick_integration_test.py
          
          # Store results in performance database (placeholder)
          echo "TODO: Store performance metrics for trend analysis"

# Schedule for weekly performance monitoring
schedule:
  - cron: '0 2 * * 1'  # Every Monday at 2 AM UTC