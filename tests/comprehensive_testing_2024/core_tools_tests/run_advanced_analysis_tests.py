#!/usr/bin/env python3
"""
ChatSpatial é«˜çº§åˆ†æå·¥å…·æµ‹è¯•è¿è¡Œå™¨

è¿™ä¸ªè„šæœ¬æŒ‰ç…§Linusçš„å“²å­¦è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼š
1. "è§£å†³çœŸå®é—®é¢˜" - æµ‹è¯•å®é™…ä½¿ç”¨åœºæ™¯
2. "æ¶ˆé™¤ç‰¹æ®Šæƒ…å†µ" - ç»Ÿä¸€çš„æµ‹è¯•æ¡†æ¶
3. "Never break userspace" - éªŒè¯æ‰€æœ‰fallbackæœºåˆ¶
4. "å¥½å“å‘³çš„ä»£ç " - ç®€æ´æœ‰æ•ˆçš„æµ‹è¯•é€»è¾‘

ç”¨æ³•:
    python run_advanced_analysis_tests.py [options]

é€‰é¡¹:
    --quick: å¿«é€Ÿæµ‹è¯•ï¼ˆè·³è¿‡è€—æ—¶æµ‹è¯•ï¼‰
    --full: å®Œæ•´æµ‹è¯•ï¼ˆåŒ…æ‹¬æ€§èƒ½åŸºå‡†ï¼‰
    --report-only: ä»…ç”Ÿæˆä¾èµ–æŠ¥å‘Š
    --output-dir: æŠ¥å‘Šè¾“å‡ºç›®å½•
"""

import sys
import time
import argparse
import logging
import asyncio
import json
from pathlib import Path
from typing import Dict, Any, Optional, List

# è®¾ç½®é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æµ‹è¯•ç»„ä»¶
from test_advanced_analysis_tools import AdvancedAnalysisTestFramework
from dependency_validator import DependencyValidator

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('advanced_analysis_test.log')
    ]
)

logger = logging.getLogger(__name__)


class AdvancedAnalysisTestRunner:
    """
    é«˜çº§åˆ†æå·¥å…·æµ‹è¯•è¿è¡Œå™¨
    
    Linuså¼è®¾è®¡åŸåˆ™ï¼š
    - å•ä¸€èŒè´£ï¼šåè°ƒæµ‹è¯•è¿è¡Œ
    - å¤±è´¥å¿«é€Ÿï¼šé‡åˆ°è‡´å‘½é”™è¯¯ç«‹å³åœæ­¢
    - æ¸…æ™°åé¦ˆï¼šå®æ—¶è¿›åº¦å’Œç»“æœæŠ¥å‘Š
    - å®ç”¨ä¸»ä¹‰ï¼šä¸“æ³¨è§£å†³å®é™…é—®é¢˜
    """
    
    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.start_time = None
        self.results = {}
        
    def run_dependency_check(self) -> Dict[str, Any]:
        """
        è¿è¡Œä¾èµ–æ£€æŸ¥
        
        è¿”å›ä¾èµ–æŠ¥å‘Šï¼Œå¦‚æœå…³é”®ä¾èµ–ç¼ºå¤±åˆ™è­¦å‘Šä½†ä¸åœæ­¢
        ï¼ˆéµå¾ªå®ç”¨ä¸»ä¹‰ï¼šå°½å¯èƒ½å¤šåœ°æµ‹è¯•å¯ç”¨åŠŸèƒ½ï¼‰
        """
        logger.info("ğŸ” Step 1: Dependency validation...")
        
        validator = DependencyValidator()
        dependency_report = validator.generate_dependency_report()
        
        # ä¿å­˜ä¾èµ–æŠ¥å‘Š
        json_path = self.output_dir / 'dependency_report.json'
        validator.save_report_json(json_path)
        
        md_path = self.output_dir / 'dependency_report.md'
        validator.save_report_markdown(md_path)
        
        # åˆ†æä¾èµ–çŠ¶æ€
        summary = dependency_report['summary']
        critical_missing = summary['critical_missing']
        
        if critical_missing:
            logger.warning(f"âš ï¸  Critical dependencies missing: {', '.join(critical_missing)}")
            logger.info("Will test available functionality with fallback mechanisms")
        else:
            logger.info("âœ… All critical dependencies available")
        
        logger.info(f"Dependency check completed: {summary['functionality_rate']:.1%} functional")
        
        return dependency_report
    
    async def run_advanced_analysis_tests(self, quick_mode: bool = False) -> Dict[str, Any]:
        """
        è¿è¡Œé«˜çº§åˆ†æå·¥å…·æµ‹è¯•
        
        å‚æ•°:
            quick_mode: å¦‚æœä¸ºTrueï¼Œè·³è¿‡è€—æ—¶çš„æµ‹è¯•
        """
        logger.info("ğŸ§ª Step 2: Advanced analysis tools testing...")
        
        framework = AdvancedAnalysisTestFramework()
        
        # è®¾ç½®æµ‹è¯•æ•°æ®
        if not framework.setup_test_data():
            raise RuntimeError("Failed to setup test data - cannot continue")
        
        logger.info(f"Test data ready: {len(framework.data_store)} datasets")
        
        # æ£€æŸ¥ä¾èµ–çŠ¶æ€
        framework.check_dependencies()
        
        # è¿è¡Œæµ‹è¯•æ¨¡å—
        test_results = {}
        
        try:
            # è½¨è¿¹åˆ†ææµ‹è¯•
            logger.info("Testing trajectory analysis...")
            trajectory_results = await framework.test_trajectory_analysis()
            test_results['trajectory_analysis'] = trajectory_results
            
        except Exception as e:
            logger.error(f"Trajectory analysis testing failed: {e}")
            test_results['trajectory_analysis'] = {'error': str(e)}
        
        try:
            # é€šè·¯å¯Œé›†åˆ†ææµ‹è¯•
            logger.info("Testing pathway enrichment...")
            pathway_results = await framework.test_pathway_enrichment()
            test_results['pathway_enrichment'] = pathway_results
            
        except Exception as e:
            logger.error(f"Pathway enrichment testing failed: {e}")
            test_results['pathway_enrichment'] = {'error': str(e)}
        
        try:
            # ç©ºé—´å¯Œé›†åˆ†ææµ‹è¯•
            logger.info("Testing spatial enrichment...")
            spatial_enrichment_results = await framework.test_spatial_enrichment()
            test_results['spatial_enrichment'] = spatial_enrichment_results
            
        except Exception as e:
            logger.error(f"Spatial enrichment testing failed: {e}")
            test_results['spatial_enrichment'] = {'error': str(e)}
        
        try:
            # ç©ºé—´é…å‡†æµ‹è¯•
            logger.info("Testing spatial registration...")
            registration_results = await framework.test_spatial_registration()
            test_results['spatial_registration'] = registration_results
            
        except Exception as e:
            logger.error(f"Spatial registration testing failed: {e}")
            test_results['spatial_registration'] = {'error': str(e)}
        
        # ç”Ÿæˆæ€§èƒ½åŸºå‡†
        performance_benchmarks = framework.generate_performance_benchmarks()
        
        return {
            'test_results': test_results,
            'performance_benchmarks': performance_benchmarks,
            'dependency_status': framework.dependency_status
        }
    
    def generate_comprehensive_report(
        self, 
        dependency_report: Dict[str, Any], 
        analysis_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š
        
        æ•´åˆä¾èµ–æ£€æŸ¥å’ŒåŠŸèƒ½æµ‹è¯•ç»“æœ
        """
        logger.info("ğŸ“Š Step 3: Generating comprehensive report...")
        
        total_time = time.time() - self.start_time if self.start_time else 0
        
        # ç»Ÿè®¡æµ‹è¯•æˆåŠŸç‡
        test_results = analysis_results.get('test_results', {})
        success_count = 0
        total_count = 0
        
        for tool_name, tool_results in test_results.items():
            for test_name, test_result in tool_results.items():
                if isinstance(test_result, dict) and 'success' in test_result:
                    total_count += 1
                    if test_result.get('success', False):
                        success_count += 1
        
        success_rate = success_count / total_count if total_count > 0 else 0
        
        # åˆ†ææ€§èƒ½æ•°æ®
        performance_data = analysis_results.get('performance_benchmarks', {})
        performance_summary = {}
        
        if 'summary' in performance_data:
            performance_summary = {
                'total_operations': performance_data['summary'].get('n_operations', 0),
                'total_test_time': performance_data['summary'].get('total_test_time', 0),
                'mean_operation_time': performance_data['summary'].get('mean_operation_time', 0),
                'slowest_operation': performance_data['summary'].get('slowest_operation_time', 0)
            }
        
        # ç”Ÿæˆæœ€ç»ˆå»ºè®®
        recommendations = self._generate_final_recommendations(
            dependency_report, analysis_results, success_rate
        )
        
        comprehensive_report = {
            'metadata': {
                'test_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_runtime': total_time,
                'python_version': sys.version.split()[0],
                'test_framework_version': '1.0.0'
            },
            'executive_summary': {
                'overall_status': self._get_overall_status(dependency_report, success_rate),
                'dependency_health': dependency_report['summary']['functionality_rate'],
                'test_success_rate': success_rate,
                'critical_issues': self._identify_critical_issues(dependency_report, analysis_results),
                'ready_for_production': success_rate > 0.8 and dependency_report['summary']['functionality_rate'] > 0.7
            },
            'dependency_analysis': dependency_report,
            'functional_testing': analysis_results,
            'performance_analysis': performance_summary,
            'recommendations': recommendations,
            'detailed_results': test_results
        }
        
        return comprehensive_report
    
    def _get_overall_status(self, dependency_report: Dict[str, Any], success_rate: float) -> str:
        """ç¡®å®šæ€»ä½“çŠ¶æ€"""
        dep_rate = dependency_report['summary']['functionality_rate']
        
        if success_rate > 0.9 and dep_rate > 0.9:
            return "EXCELLENT"
        elif success_rate > 0.7 and dep_rate > 0.7:
            return "GOOD"
        elif success_rate > 0.5 and dep_rate > 0.5:
            return "FAIR"
        elif success_rate > 0.3 or dep_rate > 0.3:
            return "POOR"
        else:
            return "CRITICAL"
    
    def _identify_critical_issues(
        self, 
        dependency_report: Dict[str, Any], 
        analysis_results: Dict[str, Any]
    ) -> List[str]:
        """è¯†åˆ«å…³é”®é—®é¢˜"""
        issues = []
        
        # å…³é”®ä¾èµ–ç¼ºå¤±
        critical_missing = dependency_report['summary'].get('critical_missing', [])
        if critical_missing:
            issues.append(f"Critical dependencies missing: {', '.join(critical_missing)}")
        
        # åŠŸèƒ½æµ‹è¯•å¤±è´¥
        test_results = analysis_results.get('test_results', {})
        failed_tools = []
        
        for tool_name, tool_results in test_results.items():
            if 'error' in tool_results:
                failed_tools.append(tool_name)
            else:
                # æ£€æŸ¥å„ä¸ªæµ‹è¯•çš„å¤±è´¥æƒ…å†µ
                failed_tests = []
                for test_name, test_result in tool_results.items():
                    if isinstance(test_result, dict) and not test_result.get('success', True):
                        failed_tests.append(test_name)
                
                if failed_tests:
                    issues.append(f"{tool_name} failed tests: {', '.join(failed_tests)}")
        
        if failed_tools:
            issues.append(f"Complete tool failures: {', '.join(failed_tools)}")
        
        return issues
    
    def _generate_final_recommendations(
        self,
        dependency_report: Dict[str, Any],
        analysis_results: Dict[str, Any],
        success_rate: float
    ) -> List[str]:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®"""
        recommendations = []
        
        # åŸºäºä¾èµ–çŠ¶æ€çš„å»ºè®®
        critical_missing = dependency_report['summary'].get('critical_missing', [])
        if critical_missing:
            rec_text = "IMMEDIATE: Install critical dependencies: "
            install_commands = []
            
            for dep in critical_missing:
                detailed_results = dependency_report.get('detailed_results', {})
                if dep in detailed_results and detailed_results[dep].get('installation_command'):
                    install_commands.append(detailed_results[dep]['installation_command'])
            
            if install_commands:
                rec_text += "; ".join(install_commands)
            else:
                rec_text += ", ".join(critical_missing)
            
            recommendations.append(rec_text)
        
        # åŸºäºæµ‹è¯•æˆåŠŸç‡çš„å»ºè®®
        if success_rate < 0.5:
            recommendations.append("URGENT: Major system issues detected - review all installations")
        elif success_rate < 0.8:
            recommendations.append("MEDIUM: Some tools non-functional - consider installing optional dependencies")
        
        # åŸºäºæ€§èƒ½çš„å»ºè®®
        performance_data = analysis_results.get('performance_benchmarks', {})
        if 'summary' in performance_data:
            total_time = performance_data['summary'].get('total_test_time', 0)
            if total_time > 300:  # 5åˆ†é’Ÿ
                recommendations.append("PERFORMANCE: Consider GPU acceleration for large-scale analysis")
            elif total_time > 120:  # 2åˆ†é’Ÿ
                recommendations.append("INFO: Performance acceptable for interactive use")
        
        # å…·ä½“å·¥å…·å»ºè®®
        test_results = analysis_results.get('test_results', {})
        
        # è½¨è¿¹åˆ†æå»ºè®®
        if 'trajectory_analysis' in test_results:
            traj_results = test_results['trajectory_analysis']
            if 'trajectory_inference' in traj_results:
                inference_results = traj_results['trajectory_inference']
                working_methods = [method for method, result in inference_results.items() 
                                 if isinstance(result, dict) and result.get('success', False)]
                
                if not working_methods:
                    recommendations.append("WARNING: No trajectory inference methods working")
                elif len(working_methods) == 1 and working_methods[0] == 'dpt':
                    recommendations.append("INFO: Only basic DPT available - consider installing CellRank or Palantir")
        
        # å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½å¾ˆå¥½
        if success_rate > 0.9 and dependency_report['summary']['functionality_rate'] > 0.9:
            recommendations.append("SUCCESS: All systems functional - ready for production use")
        
        return recommendations
    
    def save_reports(self, comprehensive_report: Dict[str, Any]) -> None:
        """ä¿å­˜æ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶"""
        logger.info("ğŸ’¾ Step 4: Saving reports...")
        
        # JSONæ ¼å¼è¯¦ç»†æŠ¥å‘Š
        json_path = self.output_dir / 'advanced_analysis_test_results.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"Detailed JSON report saved: {json_path}")
        
        # Markdownæ ¼å¼æ‘˜è¦æŠ¥å‘Š
        md_path = self.output_dir / 'advanced_analysis_test_summary.md'
        self._save_markdown_summary(comprehensive_report, md_path)
        
        logger.info(f"Summary Markdown report saved: {md_path}")
        
        # å¦‚æœæœ‰æ€§èƒ½æ•°æ®ï¼Œä¿å­˜æ€§èƒ½æŠ¥å‘Š
        if comprehensive_report.get('performance_analysis'):
            perf_path = self.output_dir / 'performance_benchmark.json'
            with open(perf_path, 'w', encoding='utf-8') as f:
                json.dump(comprehensive_report['performance_analysis'], f, indent=2)
            
            logger.info(f"Performance benchmark saved: {perf_path}")
    
    def _save_markdown_summary(self, report: Dict[str, Any], filepath: Path) -> None:
        """ä¿å­˜Markdownæ ¼å¼æ‘˜è¦"""
        summary = report['executive_summary']
        metadata = report['metadata']
        
        md_content = f"""# ChatSpatial é«˜çº§åˆ†æå·¥å…·æµ‹è¯•æ‘˜è¦

**æµ‹è¯•æ—¥æœŸ**: {metadata['test_date']}  
**è¿è¡Œæ—¶é—´**: {metadata['total_runtime']:.2f}ç§’  
**Pythonç‰ˆæœ¬**: {metadata['python_version']}  

## æ€»ä½“çŠ¶æ€: {summary['overall_status']}

- **ä¾èµ–å¥åº·åº¦**: {summary['dependency_health']:.1%}
- **æµ‹è¯•æˆåŠŸç‡**: {summary['test_success_rate']:.1%}
- **ç”Ÿäº§å°±ç»ª**: {'âœ… æ˜¯' if summary['ready_for_production'] else 'âŒ å¦'}

## å…³é”®å‘ç°

"""
        
        if summary['critical_issues']:
            md_content += "### ğŸ”´ å…³é”®é—®é¢˜\n\n"
            for issue in summary['critical_issues']:
                md_content += f"- {issue}\n"
            md_content += "\n"
        
        # å·¥å…·çŠ¶æ€æ‘˜è¦
        md_content += "### å·¥å…·æ¨¡å—çŠ¶æ€\n\n"
        test_results = report.get('detailed_results', {})
        
        for tool_name, tool_results in test_results.items():
            if 'error' in tool_results:
                status_icon = "âŒ"
                status_text = f"å¤±è´¥: {tool_results['error']}"
            else:
                # è®¡ç®—è¿™ä¸ªå·¥å…·çš„æˆåŠŸç‡
                success_count = 0
                total_count = 0
                
                for test_name, test_result in tool_results.items():
                    if isinstance(test_result, dict) and 'success' in test_result:
                        total_count += 1
                        if test_result.get('success', False):
                            success_count += 1
                
                if total_count > 0:
                    success_rate = success_count / total_count
                    if success_rate >= 1.0:
                        status_icon = "âœ…"
                        status_text = "å®Œå…¨æ­£å¸¸"
                    elif success_rate >= 0.5:
                        status_icon = "âš ï¸"
                        status_text = f"éƒ¨åˆ†æ­£å¸¸ ({success_count}/{total_count})"
                    else:
                        status_icon = "âŒ"
                        status_text = f"å¤§éƒ¨åˆ†å¤±è´¥ ({success_count}/{total_count})"
                else:
                    status_icon = "â“"
                    status_text = "æœªæµ‹è¯•"
            
            tool_display_name = tool_name.replace('_', ' ').title()
            md_content += f"- {status_icon} **{tool_display_name}**: {status_text}\n"
        
        # æ€§èƒ½æ‘˜è¦
        if 'performance_analysis' in report and report['performance_analysis']:
            perf = report['performance_analysis']
            md_content += f"\n### æ€§èƒ½æ¦‚è§ˆ\n\n"
            md_content += f"- **æ€»æ“ä½œæ•°**: {perf.get('total_operations', 0)}\n"
            md_content += f"- **æµ‹è¯•æ€»æ—¶é—´**: {perf.get('total_test_time', 0):.2f}ç§’\n"
            md_content += f"- **å¹³å‡æ“ä½œæ—¶é—´**: {perf.get('mean_operation_time', 0):.3f}ç§’\n"
            md_content += f"- **æœ€æ…¢æ“ä½œæ—¶é—´**: {perf.get('slowest_operation', 0):.3f}ç§’\n"
        
        # å»ºè®®
        if report.get('recommendations'):
            md_content += "\n## å»ºè®®\n\n"
            for i, rec in enumerate(report['recommendations'], 1):
                priority = "ğŸ”´" if rec.startswith(('IMMEDIATE', 'URGENT', 'CRITICAL')) else \
                          "ğŸŸ¡" if rec.startswith(('MEDIUM', 'WARNING')) else \
                          "â„¹ï¸"
                md_content += f"{i}. {priority} {rec}\n"
        
        md_content += f"\n---\n\n**è¯¦ç»†æŠ¥å‘Š**: `advanced_analysis_test_results.json`\n"
        md_content += f"**ä¾èµ–æŠ¥å‘Š**: `dependency_report.md`\n"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(md_content)
    
    async def run_complete_test_suite(self, quick_mode: bool = False) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
        
        è¿™æ˜¯ä¸»è¦çš„åè°ƒå‡½æ•°ï¼ŒæŒ‰ç…§Linusçš„åŸåˆ™ï¼š
        1. å¤±è´¥æ—¶å¿«é€Ÿåœæ­¢
        2. æ¸…æ™°çš„è¿›åº¦åé¦ˆ
        3. å…¨é¢çš„é”™è¯¯è®°å½•
        """
        self.start_time = time.time()
        
        logger.info("ğŸš€ Starting ChatSpatial Advanced Analysis Tools Test Suite")
        logger.info(f"Output directory: {self.output_dir}")
        
        try:
            # æ­¥éª¤1ï¼šä¾èµ–æ£€æŸ¥
            dependency_report = self.run_dependency_check()
            
            # æ­¥éª¤2ï¼šåŠŸèƒ½æµ‹è¯•
            analysis_results = await self.run_advanced_analysis_tests(quick_mode)
            
            # æ­¥éª¤3ï¼šç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = self.generate_comprehensive_report(
                dependency_report, analysis_results
            )
            
            # æ­¥éª¤4ï¼šä¿å­˜æŠ¥å‘Š
            self.save_reports(comprehensive_report)
            
            # æœ€ç»ˆçŠ¶æ€
            total_time = time.time() - self.start_time
            status = comprehensive_report['executive_summary']['overall_status']
            
            logger.info(f"ğŸ¯ Test suite completed in {total_time:.2f}s")
            logger.info(f"Overall status: {status}")
            
            if comprehensive_report['executive_summary']['ready_for_production']:
                logger.info("âœ… System ready for production use")
            else:
                logger.warning("âš ï¸ System needs attention before production use")
            
            return comprehensive_report
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Test suite failed: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            # ä¿å­˜é”™è¯¯æŠ¥å‘Š
            error_report = {
                'error': str(e),
                'traceback': traceback.format_exc(),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'runtime': time.time() - self.start_time if self.start_time else 0
            }
            
            error_path = self.output_dir / 'test_suite_error.json'
            with open(error_path, 'w') as f:
                json.dump(error_report, f, indent=2)
            
            raise
    
    def print_summary(self, report: Dict[str, Any]) -> None:
        """æ‰“å°æµ‹è¯•æ‘˜è¦åˆ°æ§åˆ¶å°"""
        print("\n" + "="*80)
        print("ChatSpatial é«˜çº§åˆ†æå·¥å…·æµ‹è¯•æ‘˜è¦")
        print("="*80)
        
        summary = report['executive_summary']
        metadata = report['metadata']
        
        # æ€»ä½“çŠ¶æ€
        status_colors = {
            'EXCELLENT': 'ğŸŸ¢',
            'GOOD': 'ğŸŸ¢',
            'FAIR': 'ğŸŸ¡',
            'POOR': 'ğŸŸ ',
            'CRITICAL': 'ğŸ”´'
        }
        
        status_icon = status_colors.get(summary['overall_status'], 'â“')
        print(f"æ€»ä½“çŠ¶æ€: {status_icon} {summary['overall_status']}")
        print(f"æµ‹è¯•æ—¶é—´: {metadata['total_runtime']:.2f}ç§’")
        print(f"ä¾èµ–å¥åº·åº¦: {summary['dependency_health']:.1%}")
        print(f"æµ‹è¯•æˆåŠŸç‡: {summary['test_success_rate']:.1%}")
        print(f"ç”Ÿäº§å°±ç»ª: {'âœ…' if summary['ready_for_production'] else 'âŒ'}")
        
        # å…³é”®é—®é¢˜
        if summary['critical_issues']:
            print(f"\nğŸ”´ å…³é”®é—®é¢˜:")
            for issue in summary['critical_issues']:
                print(f"  â€¢ {issue}")
        
        # å»ºè®®
        if report.get('recommendations'):
            print(f"\nğŸ“‹ ä¸»è¦å»ºè®®:")
            for i, rec in enumerate(report['recommendations'][:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  {i}. {rec}")
            
            if len(report['recommendations']) > 3:
                print(f"  ... è¿˜æœ‰ {len(report['recommendations']) - 3} æ¡å»ºè®®ï¼ˆè§è¯¦ç»†æŠ¥å‘Šï¼‰")
        
        print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.output_dir}")
        print("="*80)


async def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(
        description="ChatSpatial Advanced Analysis Tools Test Suite",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_advanced_analysis_tests.py --full
  python run_advanced_analysis_tests.py --quick --output-dir ./test_results
  python run_advanced_analysis_tests.py --report-only
        """
    )
    
    parser.add_argument('--quick', action='store_true', 
                       help='Run quick tests only (skip time-consuming benchmarks)')
    parser.add_argument('--full', action='store_true',
                       help='Run full test suite including performance benchmarks')
    parser.add_argument('--report-only', action='store_true',
                       help='Generate dependency report only (no functional testing)')
    parser.add_argument('--output-dir', type=Path, 
                       default=Path('./advanced_analysis_test_results'),
                       help='Output directory for test reports (default: ./advanced_analysis_test_results)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = AdvancedAnalysisTestRunner(args.output_dir)
    
    try:
        if args.report_only:
            # ä»…ç”Ÿæˆä¾èµ–æŠ¥å‘Š
            logger.info("Running dependency check only...")
            dependency_report = runner.run_dependency_check()
            
            print("\n" + "="*60)
            print("DEPENDENCY CHECK SUMMARY")
            print("="*60)
            
            summary = dependency_report['summary']
            print(f"Dependencies: {summary['functional_dependencies']}/{summary['total_dependencies']} functional")
            
            if summary['critical_missing']:
                print(f"Critical missing: {', '.join(summary['critical_missing'])}")
            
            print(f"Report saved to: {args.output_dir}")
            print("="*60)
            
        else:
            # è¿è¡Œå®Œæ•´æµ‹è¯•
            quick_mode = args.quick or not args.full
            
            if quick_mode:
                logger.info("Running in quick mode (skipping intensive benchmarks)")
            else:
                logger.info("Running full test suite including performance benchmarks")
            
            comprehensive_report = await runner.run_complete_test_suite(quick_mode)
            runner.print_summary(comprehensive_report)
            
            # è¿”å›é€‚å½“çš„é€€å‡ºä»£ç 
            if comprehensive_report['executive_summary']['ready_for_production']:
                return 0
            elif comprehensive_report['executive_summary']['overall_status'] in ['POOR', 'CRITICAL']:
                return 2
            else:
                return 1
    
    except KeyboardInterrupt:
        logger.info("Test interrupted by user")
        return 130
    
    except Exception as e:
        logger.error(f"Test suite failed: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(asyncio.run(main()))