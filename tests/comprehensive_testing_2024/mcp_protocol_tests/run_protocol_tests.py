"""
ChatSpatial MCP åè®®æµ‹è¯•è¿è¡Œå™¨

æœ¬è„šæœ¬æä¾›å®Œæ•´çš„MCPåè®®å±‚æµ‹è¯•å¥—ä»¶æ‰§è¡Œå™¨ï¼ŒåŒ…æ‹¬æ€§èƒ½åŸºå‡†ã€è¯¦ç»†æŠ¥å‘Šç”Ÿæˆå’Œæµ‹è¯•ç»“æœæ±‡æ€»ã€‚
è¿™æ˜¯Linusé£æ ¼çš„æµ‹è¯•è¿è¡Œå™¨ - ç®€å•ã€ç›´æ¥ã€æ— åºŸè¯ã€‚

åŠŸèƒ½ï¼š
1. è¿è¡Œæ‰€æœ‰5ä¸ªæµ‹è¯•æ¨¡å—
2. ç”Ÿæˆæ€§èƒ½åŸºå‡†æŠ¥å‘Š
3. æ±‡æ€»æµ‹è¯•ç»“æœå’ŒæŒ‡æ ‡
4. ä¿å­˜è¯¦ç»†çš„JSONå’ŒHTMLæŠ¥å‘Š
5. æä¾›CI/CDå‹å¥½çš„é€€å‡ºä»£ç 

ä½¿ç”¨æ–¹å¼ï¼š
    python run_protocol_tests.py [--verbose] [--performance] [--report-dir DIR]

ä½œè€…: Linus é£æ ¼çš„æµ‹è¯•è‡ªåŠ¨åŒ–
"""

import argparse
import json
import sys
import time
import subprocess
import importlib.util
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import traceback
import os

# æµ‹è¯•é…ç½®
PROTOCOL_TEST_MODULES = [
    "test_server_startup",
    "test_tool_registration", 
    "test_parameter_validation",
    "test_error_responses",
    "test_http_transport"
]

class ProtocolTestRunner:
    """MCPåè®®æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, verbose: bool = False, report_dir: Optional[Path] = None):
        self.verbose = verbose
        self.report_dir = report_dir or Path(__file__).parent
        self.report_dir.mkdir(exist_ok=True)
        
        self.test_results = {}
        self.performance_metrics = {}
        self.start_time = time.time()
        self.test_modules = PROTOCOL_TEST_MODULES  # é»˜è®¤ä½¿ç”¨æ‰€æœ‰æ¨¡å—
        
        # ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨
        self.reports_dir = self.report_dir / "reports"
        self.reports_dir.mkdir(exist_ok=True)
    
    def log(self, message: str, level: str = "INFO"):
        """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        prefix = f"[{timestamp}] {level}:"
        
        if self.verbose or level in ["ERROR", "WARNING"]:
            print(f"{prefix} {message}")
    
    def run_single_test_module(self, module_name: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ¨¡å—"""
        self.log(f"Running {module_name}...")
        
        module_start = time.time()
        
        try:
            # ä½¿ç”¨pytestè¿è¡Œç‰¹å®šæ¨¡å—
            test_file = Path(__file__).parent / f"{module_name}.py"
            
            if not test_file.exists():
                return {
                    "status": "FAILED",
                    "error": f"Test file {test_file} not found",
                    "execution_time": 0,
                    "tests_run": 0,
                    "tests_passed": 0,
                    "tests_failed": 1
                }
            
            # è¿è¡Œpytestå¹¶æ•è·è¾“å‡º
            cmd = [
                sys.executable, "-m", "pytest", 
                str(test_file),
                "-v",
                "--tb=short",
                "--json-report",
                f"--json-report-file={self.reports_dir / f'{module_name}_report.json'}"
            ]
            
            if not self.verbose:
                cmd.extend(["--quiet"])
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            module_time = time.time() - module_start
            
            # è§£æpytestç»“æœ
            json_report_path = self.reports_dir / f"{module_name}_report.json"
            if json_report_path.exists():
                with open(json_report_path, 'r') as f:
                    pytest_report = json.load(f)
                
                # ä»pytestæŠ¥å‘Šæå–ä¿¡æ¯
                summary = pytest_report.get("summary", {})
                tests_run = summary.get("total", 0)
                tests_passed = summary.get("passed", 0)
                tests_failed = summary.get("failed", 0)
                tests_error = summary.get("error", 0)
                
                status = "PASSED" if tests_failed == 0 and tests_error == 0 else "FAILED"
            else:
                # Fallback: æ ¹æ®é€€å‡ºä»£ç åˆ¤æ–­
                status = "PASSED" if result.returncode == 0 else "FAILED"
                tests_run = 1  # è‡³å°‘è¿è¡Œäº†æŸäº›æµ‹è¯•
                tests_passed = 1 if status == "PASSED" else 0
                tests_failed = 0 if status == "PASSED" else 1
            
            module_result = {
                "status": status,
                "execution_time": module_time,
                "tests_run": tests_run,
                "tests_passed": tests_passed,
                "tests_failed": tests_failed,
                "stdout": result.stdout[-1000:] if result.stdout else "",  # æœ€å1000å­—ç¬¦
                "stderr": result.stderr[-1000:] if result.stderr else ""
            }
            
            if result.returncode != 0 and not json_report_path.exists():
                module_result["error"] = f"pytest exited with code {result.returncode}"
            
            self.log(f"Completed {module_name}: {status} ({tests_passed}/{tests_run} passed, {module_time:.2f}s)")
            
            return module_result
            
        except subprocess.TimeoutExpired:
            return {
                "status": "TIMEOUT",
                "error": f"Test module {module_name} timed out after 300s",
                "execution_time": time.time() - module_start,
                "tests_run": 0,
                "tests_passed": 0,
                "tests_failed": 1
            }
        except Exception as e:
            return {
                "status": "ERROR",
                "error": f"Exception running {module_name}: {str(e)}",
                "execution_time": time.time() - module_start,
                "tests_run": 0,
                "tests_passed": 0,
                "tests_failed": 1
            }
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰åè®®æµ‹è¯•"""
        self.log("Starting ChatSpatial MCP Protocol Test Suite...")
        self.log(f"Test modules: {', '.join(self.test_modules)}")
        
        overall_results = {
            "start_time": datetime.now().isoformat(),
            "modules": {},
            "summary": {
                "total_modules": len(self.test_modules),
                "modules_passed": 0,
                "modules_failed": 0,
                "total_tests": 0,
                "total_passed": 0,
                "total_failed": 0,
                "total_execution_time": 0
            }
        }
        
        # è¿è¡Œæ¯ä¸ªæµ‹è¯•æ¨¡å—
        for module_name in self.test_modules:
            module_result = self.run_single_test_module(module_name)
            overall_results["modules"][module_name] = module_result
            
            # æ›´æ–°æ±‡æ€»ç»Ÿè®¡
            if module_result["status"] == "PASSED":
                overall_results["summary"]["modules_passed"] += 1
            else:
                overall_results["summary"]["modules_failed"] += 1
            
            overall_results["summary"]["total_tests"] += module_result.get("tests_run", 0)
            overall_results["summary"]["total_passed"] += module_result.get("tests_passed", 0)
            overall_results["summary"]["total_failed"] += module_result.get("tests_failed", 0)
            overall_results["summary"]["total_execution_time"] += module_result.get("execution_time", 0)
        
        overall_results["end_time"] = datetime.now().isoformat()
        overall_results["total_wall_time"] = time.time() - self.start_time
        
        return overall_results
    
    def generate_performance_benchmark(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½åŸºå‡†æŠ¥å‘Š"""
        self.log("Generating performance benchmark...")
        
        benchmark = {
            "timestamp": datetime.now().isoformat(),
            "system_info": self.get_system_info(),
            "benchmarks": {}
        }
        
        # ä»å„ä¸ªæµ‹è¯•æ¨¡å—æ”¶é›†æ€§èƒ½æ•°æ®
        perf_files = list(self.reports_dir.glob("*performance*.json"))
        
        for perf_file in perf_files:
            try:
                with open(perf_file, 'r') as f:
                    perf_data = json.load(f)
                    benchmark["benchmarks"][perf_file.stem] = perf_data
            except Exception as e:
                self.log(f"Failed to load performance data from {perf_file}: {e}", "WARNING")
        
        # æ·»åŠ ä¸€äº›åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
        benchmark["benchmarks"]["test_suite_overall"] = {
            "total_execution_time": time.time() - self.start_time,
            "modules_tested": len(self.test_modules),
            "avg_module_time": (time.time() - self.start_time) / len(self.test_modules) if self.test_modules else 0
        }
        
        return benchmark
    
    def get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import platform
        import psutil
        
        return {
            "python_version": platform.python_version(),
            "platform": platform.platform(),
            "processor": platform.processor(),
            "cpu_count": psutil.cpu_count(),
            "memory_gb": round(psutil.virtual_memory().total / (1024**3), 2),
            "hostname": platform.node()
        }
    
    def generate_html_report(self, results: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>ChatSpatial MCP Protocol Test Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .summary {{ background: #e8f5e8; padding: 15px; margin: 20px 0; border-radius: 5px; }}
        .module {{ background: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .passed {{ color: #008000; font-weight: bold; }}
        .failed {{ color: #cc0000; font-weight: bold; }}
        .timeout {{ color: #ff8800; font-weight: bold; }}
        .error {{ color: #cc0000; font-weight: bold; }}
        pre {{ background: #f0f0f0; padding: 10px; overflow-x: auto; font-size: 12px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ChatSpatial MCP Protocol Test Report</h1>
        <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p><strong>Total Execution Time:</strong> {results.get('total_wall_time', 0):.2f} seconds</p>
    </div>
    
    <div class="summary">
        <h2>Test Summary</h2>
        <table>
            <tr><th>Metric</th><th>Value</th></tr>
            <tr><td>Total Modules</td><td>{results['summary']['total_modules']}</td></tr>
            <tr><td>Modules Passed</td><td class="passed">{results['summary']['modules_passed']}</td></tr>
            <tr><td>Modules Failed</td><td class="failed">{results['summary']['modules_failed']}</td></tr>
            <tr><td>Total Tests</td><td>{results['summary']['total_tests']}</td></tr>
            <tr><td>Tests Passed</td><td class="passed">{results['summary']['total_passed']}</td></tr>
            <tr><td>Tests Failed</td><td class="failed">{results['summary']['total_failed']}</td></tr>
            <tr><td>Success Rate</td><td>{(results['summary']['total_passed'] / max(results['summary']['total_tests'], 1) * 100):.1f}%</td></tr>
        </table>
    </div>
    
    <h2>Module Results</h2>
"""
        
        # ä¸ºæ¯ä¸ªæ¨¡å—æ·»åŠ è¯¦ç»†ç»“æœ
        for module_name, module_result in results["modules"].items():
            status_class = module_result["status"].lower()
            html_content += f"""
    <div class="module">
        <h3>{module_name} - <span class="{status_class}">{module_result["status"]}</span></h3>
        <p><strong>Execution Time:</strong> {module_result.get('execution_time', 0):.2f} seconds</p>
        <p><strong>Tests:</strong> {module_result.get('tests_passed', 0)}/{module_result.get('tests_run', 0)} passed</p>
        
        {f'<p><strong>Error:</strong> <code>{module_result["error"]}</code></p>' if "error" in module_result else ''}
        
        {f'<details><summary>Standard Output</summary><pre>{module_result["stdout"]}</pre></details>' if module_result.get("stdout") else ''}
        {f'<details><summary>Standard Error</summary><pre>{module_result["stderr"]}</pre></details>' if module_result.get("stderr") else ''}
    </div>
"""
        
        html_content += """
</body>
</html>"""
        
        return html_content
    
    def save_reports(self, results: Dict[str, Any], benchmark: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_report_path = self.reports_dir / f"protocol_test_report_{timestamp}.json"
        with open(json_report_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # ä¿å­˜æ€§èƒ½åŸºå‡†
        benchmark_path = self.reports_dir / f"performance_benchmark_{timestamp}.json"
        with open(benchmark_path, 'w') as f:
            json.dump(benchmark, f, indent=2, default=str)
        
        # ä¿å­˜HTMLæŠ¥å‘Š
        html_report_path = self.reports_dir / f"protocol_test_report_{timestamp}.html"
        with open(html_report_path, 'w') as f:
            f.write(self.generate_html_report(results))
        
        # ä¿å­˜æœ€æ–°æŠ¥å‘Šé“¾æ¥
        latest_json = self.reports_dir / "latest_report.json"
        latest_html = self.reports_dir / "latest_report.html"
        latest_benchmark = self.reports_dir / "latest_benchmark.json"
        
        # åˆ›å»ºç¬¦å·é“¾æ¥æˆ–å¤åˆ¶æ–‡ä»¶
        try:
            if latest_json.exists():
                latest_json.unlink()
            if latest_html.exists():
                latest_html.unlink()  
            if latest_benchmark.exists():
                latest_benchmark.unlink()
            
            # åœ¨Windowsä¸Šï¼Œä½¿ç”¨å¤åˆ¶ï¼›åœ¨Unixä¸Šï¼Œä½¿ç”¨ç¬¦å·é“¾æ¥
            if os.name == 'nt':
                import shutil
                shutil.copy2(json_report_path, latest_json)
                shutil.copy2(html_report_path, latest_html)
                shutil.copy2(benchmark_path, latest_benchmark)
            else:
                latest_json.symlink_to(json_report_path.name)
                latest_html.symlink_to(html_report_path.name)
                latest_benchmark.symlink_to(benchmark_path.name)
        except Exception as e:
            self.log(f"Failed to create latest report links: {e}", "WARNING")
        
        self.log(f"Reports saved:")
        self.log(f"  JSON: {json_report_path}")
        self.log(f"  HTML: {html_report_path}")
        self.log(f"  Benchmark: {benchmark_path}")
        
        return {
            "json_report": json_report_path,
            "html_report": html_report_path,
            "benchmark_report": benchmark_path
        }
    
    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\\n" + "="*70)
        print("CHATSPATIAL MCP PROTOCOL TEST RESULTS")
        print("="*70)
        
        summary = results["summary"]
        
        print(f"Modules: {summary['modules_passed']}/{summary['total_modules']} passed")
        print(f"Tests:   {summary['total_passed']}/{summary['total_tests']} passed")
        print(f"Time:    {summary['total_execution_time']:.2f} seconds")
        
        success_rate = (summary['total_passed'] / max(summary['total_tests'], 1)) * 100
        print(f"Success: {success_rate:.1f}%")
        
        print("\\nModule Results:")
        for module_name, module_result in results["modules"].items():
            status_symbol = "âœ…" if module_result["status"] == "PASSED" else "âŒ"
            print(f"  {status_symbol} {module_name}: {module_result['status']} "
                  f"({module_result.get('tests_passed', 0)}/{module_result.get('tests_run', 0)} tests, "
                  f"{module_result.get('execution_time', 0):.2f}s)")
            
            if "error" in module_result:
                print(f"     Error: {module_result['error'][:100]}...")
        
        if summary['modules_failed'] == 0:
            print("\\nğŸ‰ ALL PROTOCOL TESTS PASSED! MCPåè®®å±‚å¥åº·ã€‚")
        else:
            print(f"\\nâš ï¸  {summary['modules_failed']} modules failed. æ£€æŸ¥è¯¦ç»†æŠ¥å‘Šã€‚")
        
        print("="*70)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Run ChatSpatial MCP Protocol Tests")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    parser.add_argument("--performance", "-p", action="store_true", help="Include performance tests")
    parser.add_argument("--report-dir", type=Path, help="Report output directory")
    parser.add_argument("--modules", nargs="+", choices=PROTOCOL_TEST_MODULES, 
                       help="Specific modules to run")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = ProtocolTestRunner(
        verbose=args.verbose,
        report_dir=args.report_dir
    )
    
    try:
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šæ¨¡å—ï¼Œåªè¿è¡Œé‚£äº›æ¨¡å—
        modules_to_run = args.modules if args.modules else PROTOCOL_TEST_MODULES
        if args.modules:
            runner.log(f"Running specific modules: {args.modules}")
        
        # æ›´æ–°è¿è¡Œå™¨ä½¿ç”¨çš„æ¨¡å—åˆ—è¡¨
        runner.test_modules = modules_to_run
        
        # è¿è¡Œæµ‹è¯•
        results = runner.run_all_tests()
        
        # ç”Ÿæˆæ€§èƒ½åŸºå‡†
        benchmark = runner.generate_performance_benchmark()
        
        # ä¿å­˜æŠ¥å‘Š
        report_paths = runner.save_reports(results, benchmark)
        
        # æ‰“å°æ‘˜è¦
        runner.print_summary(results)
        
        # è¿”å›é€‚å½“çš„é€€å‡ºä»£ç 
        if results["summary"]["modules_failed"] == 0:
            runner.log("All protocol tests passed successfully!")
            sys.exit(0)
        else:
            runner.log(f"{results['summary']['modules_failed']} modules failed", "ERROR")
            sys.exit(1)
            
    except KeyboardInterrupt:
        runner.log("Test execution interrupted by user", "WARNING")
        sys.exit(130)
    except Exception as e:
        runner.log(f"Test runner failed with exception: {str(e)}", "ERROR")
        if args.verbose:
            runner.log(traceback.format_exc(), "ERROR")
        sys.exit(1)


if __name__ == "__main__":
    main()