#!/usr/bin/env python3
"""
æ·±åº¦æ•´ç† ChatSpatial data ç›®å½•
- å¤„ç†é‡å¤æ–‡ä»¶
- æ¸…ç†æ—§ç›®å½•ç»“æ„  
- è§„èŒƒå‘½å
- é‡æ–°åˆ†ç±»æ•°æ®é›†
- åˆ›å»ºå®Œæ•´ç´¢å¼•
"""

import os
import shutil
from pathlib import Path
import json
import scanpy as sc
import pandas as pd
import warnings
from collections import defaultdict
import hashlib
warnings.filterwarnings('ignore')

class DataDirectoryCleaner:
    """æ·±åº¦æ¸…ç†dataç›®å½•"""
    
    def __init__(self, data_dir):
        self.data_dir = Path(data_dir)
        self.duplicates_found = []
        self.moved_files = []
        self.removed_files = []
        self.errors = []
        
        # æ ‡å‡†ç›®å½•ç»“æ„
        self.standard_dirs = {
            'spatial_datasets': 'ç©ºé—´è½¬å½•ç»„æ•°æ®é›†',
            'single_cell_datasets': 'å•ç»†èƒRNA-seqæ•°æ®é›†', 
            'synthetic_datasets': 'åˆæˆå’Œæ¨¡æ‹Ÿæ•°æ®é›†',
            'demo_datasets': 'æ¼”ç¤ºå’Œæ•™ç¨‹æ•°æ®é›†',
            'benchmark_datasets': 'æ€§èƒ½åŸºå‡†æµ‹è¯•æ•°æ®é›†',
            'reference_datasets': 'å‚è€ƒå’Œæ ‡å‡†æ•°æ®é›†',
            'metadata': 'å…ƒæ•°æ®å’Œç´¢å¼•æ–‡ä»¶',
            'documentation': 'æ–‡æ¡£å’Œè¯´æ˜',
            'scripts': 'æ•°æ®å¤„ç†è„šæœ¬',
            'archive': 'å½’æ¡£å’Œå¤‡ä»½æ–‡ä»¶'
        }
    
    def find_duplicate_files(self):
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
        print("ğŸ” æŸ¥æ‰¾é‡å¤æ–‡ä»¶...")
        
        file_hashes = defaultdict(list)
        file_sizes = defaultdict(list)
        
        # æ”¶é›†æ‰€æœ‰h5adæ–‡ä»¶
        for h5ad_file in self.data_dir.rglob("*.h5ad"):
            if h5ad_file.is_file():
                # è®¡ç®—æ–‡ä»¶å¤§å°
                size = h5ad_file.stat().st_size
                file_sizes[size].append(h5ad_file)
        
        # å¯¹ç›¸åŒå¤§å°çš„æ–‡ä»¶è®¡ç®—å“ˆå¸Œ
        for size, files in file_sizes.items():
            if len(files) > 1:
                for file_path in files:
                    try:
                        with open(file_path, 'rb') as f:
                            # åªè¯»å–å‰1MBæ¥è®¡ç®—å“ˆå¸Œï¼ŒåŠ é€Ÿå¤„ç†
                            content = f.read(1024 * 1024)
                            hash_value = hashlib.md5(content).hexdigest()
                            file_hashes[hash_value].append(file_path)
                    except Exception as e:
                        self.errors.append(f"è®¡ç®—å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
        
        # æ‰¾å‡ºçœŸæ­£çš„é‡å¤æ–‡ä»¶
        for hash_value, files in file_hashes.items():
            if len(files) > 1:
                self.duplicates_found.append({
                    'hash': hash_value,
                    'files': files,
                    'size_mb': files[0].stat().st_size / (1024*1024)
                })
                print(f"  å‘ç°é‡å¤: {[f.name for f in files]}")
        
        print(f"æ‰¾åˆ° {len(self.duplicates_found)} ç»„é‡å¤æ–‡ä»¶")
        
    def handle_duplicates(self):
        """å¤„ç†é‡å¤æ–‡ä»¶"""
        print("ğŸ§¹ å¤„ç†é‡å¤æ–‡ä»¶...")
        
        for dup_group in self.duplicates_found:
            files = dup_group['files']
            
            # é€‰æ‹©ä¿ç•™çš„æ–‡ä»¶ï¼ˆä¼˜å…ˆé€‰æ‹©åœ¨æ­£ç¡®åˆ†ç±»ç›®å½•ä¸­çš„ï¼‰
            keep_file = None
            remove_files = []
            
            # ä¼˜å…ˆçº§ï¼šæ ‡å‡†ç›®å½• > ébackup > æ–‡ä»¶åç®€æ´
            for file_path in files:
                is_in_standard_dir = any(std_dir in str(file_path) for std_dir in self.standard_dirs.keys())
                is_backup = 'backup' in file_path.name.lower()
                
                if keep_file is None:
                    keep_file = file_path
                else:
                    # å½“å‰æ–‡ä»¶æ›´å¥½çš„æ¡ä»¶
                    current_better = (
                        is_in_standard_dir and not any(std_dir in str(keep_file) for std_dir in self.standard_dirs.keys()) or
                        not is_backup and 'backup' in keep_file.name.lower() or
                        len(file_path.name) < len(keep_file.name)
                    )
                    
                    if current_better:
                        remove_files.append(keep_file)
                        keep_file = file_path
                    else:
                        remove_files.append(file_path)
            
            # ç§»åŠ¨é‡å¤æ–‡ä»¶åˆ°archive
            archive_dir = self.data_dir / 'archive' / 'duplicates'
            archive_dir.mkdir(parents=True, exist_ok=True)
            
            for remove_file in remove_files:
                archive_path = archive_dir / remove_file.name
                counter = 1
                while archive_path.exists():
                    stem = remove_file.stem
                    suffix = remove_file.suffix
                    archive_path = archive_dir / f"{stem}_dup{counter}{suffix}"
                    counter += 1
                
                try:
                    shutil.move(remove_file, archive_path)
                    self.removed_files.append(str(remove_file))
                    print(f"  å½’æ¡£é‡å¤æ–‡ä»¶: {remove_file.name} -> archive/duplicates/")
                except Exception as e:
                    self.errors.append(f"å½’æ¡£å¤±è´¥ {remove_file}: {e}")
    
    def categorize_dataset_smart(self, file_path):
        """æ™ºèƒ½åˆ†ç±»æ•°æ®é›†"""
        filename = file_path.name.lower()
        
        try:
            # å°è¯•è¯»å–æ•°æ®é›†è·å–æ›´å¤šä¿¡æ¯
            adata = sc.read_h5ad(file_path)
            has_spatial = 'spatial' in adata.obsm
            n_cells = adata.n_obs
            n_genes = adata.n_vars
            
        except:
            has_spatial = False
            n_cells = 0
            n_genes = 0
        
        # æ™ºèƒ½åˆ†ç±»è§„åˆ™
        if any(x in filename for x in ['benchmark', 'perf', 'stress']) or (100 <= n_cells <= 5000 and 500 <= n_genes <= 5000):
            return 'benchmark_datasets'
        elif any(x in filename for x in ['demo', 'tutorial', 'quick', 'example']) or 'mop_sn_tutorial' in filename:
            return 'demo_datasets'
        elif any(x in filename for x in ['synthetic', 'simulated', 'artificial', 'generated']):
            return 'synthetic_datasets'
        elif any(x in filename for x in ['paul15', 'reference', 'standard']) or n_cells > 0 and n_cells < 1000 and 'test' in filename:
            return 'reference_datasets'
        elif has_spatial or any(x in filename for x in ['visium', 'slideseq', 'merfish', 'seqfish', 'spatial', 'st_']):
            return 'spatial_datasets'
        elif n_cells > 0 and not has_spatial:
            return 'single_cell_datasets'
        else:
            return 'spatial_datasets'  # é»˜è®¤å½’ç±»ä¸ºç©ºé—´æ•°æ®
    
    def reorganize_datasets(self):
        """é‡æ–°ç»„ç»‡æ•°æ®é›†"""
        print("ğŸ“ é‡æ–°ç»„ç»‡æ•°æ®é›†åˆ†ç±»...")
        
        # åˆ›å»ºæ ‡å‡†ç›®å½•
        for dir_name, description in self.standard_dirs.items():
            dir_path = self.data_dir / dir_name
            dir_path.mkdir(exist_ok=True)
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦åˆ†ç±»çš„æ–‡ä»¶
        files_to_process = []
        
        # ä»æ—§çš„åˆ†ç±»ç›®å½•æ”¶é›†æ–‡ä»¶
        old_dirs = ['real_datasets', 'benchmark_datasets', 'demo_datasets', 'synthetic_datasets', 'reference_datasets']
        
        for old_dir in old_dirs:
            old_path = self.data_dir / old_dir
            if old_path.exists():
                for h5ad_file in old_path.glob("*.h5ad"):
                    files_to_process.append(h5ad_file)
        
        # å¤„ç†æ ¹ç›®å½•å’Œå…¶ä»–ä½ç½®çš„æ–‡ä»¶
        for h5ad_file in self.data_dir.rglob("*.h5ad"):
            if h5ad_file.parent.name not in self.standard_dirs and h5ad_file not in files_to_process:
                files_to_process.append(h5ad_file)
        
        # é‡æ–°åˆ†ç±»æ¯ä¸ªæ–‡ä»¶
        moved_count = 0
        for file_path in files_to_process:
            if not file_path.exists():
                continue
                
            new_category = self.categorize_dataset_smart(file_path)
            target_dir = self.data_dir / new_category
            target_path = target_dir / file_path.name
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»åŠ¨
            if file_path.parent != target_dir:
                # å¤„ç†åŒåæ–‡ä»¶
                if target_path.exists():
                    counter = 1
                    stem = file_path.stem
                    suffix = file_path.suffix
                    while target_path.exists():
                        target_path = target_dir / f"{stem}_v{counter}{suffix}"
                        counter += 1
                
                try:
                    shutil.move(file_path, target_path)
                    self.moved_files.append(f"{file_path.name} -> {new_category}/")
                    moved_count += 1
                    print(f"  ç§»åŠ¨: {file_path.name} -> {new_category}/")
                except Exception as e:
                    self.errors.append(f"ç§»åŠ¨å¤±è´¥ {file_path}: {e}")
        
        print(f"é‡æ–°åˆ†ç±»ç§»åŠ¨äº† {moved_count} ä¸ªæ–‡ä»¶")
    
    def clean_old_directories(self):
        """æ¸…ç†æ—§çš„ç›®å½•ç»“æ„"""
        print("ğŸ—‚ï¸ æ¸…ç†æ—§ç›®å½•ç»“æ„...")
        
        old_dirs_to_clean = ['real_datasets', 'core', 'paul15', 'test', 'test_datasets']
        archive_dir = self.data_dir / 'archive'
        archive_dir.mkdir(exist_ok=True)
        
        for old_dir_name in old_dirs_to_clean:
            old_dir = self.data_dir / old_dir_name
            if old_dir.exists():
                # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©ºæˆ–åªå«éh5adæ–‡ä»¶
                h5ad_files = list(old_dir.rglob("*.h5ad"))
                
                if not h5ad_files:
                    # ç›®å½•æ²¡æœ‰h5adæ–‡ä»¶ï¼Œå¯ä»¥ç§»åŠ¨åˆ°archive
                    archive_path = archive_dir / f"old_{old_dir_name}"
                    if archive_path.exists():
                        shutil.rmtree(archive_path)
                    
                    try:
                        shutil.move(old_dir, archive_path)
                        print(f"  å½’æ¡£æ—§ç›®å½•: {old_dir_name} -> archive/")
                    except Exception as e:
                        self.errors.append(f"å½’æ¡£ç›®å½•å¤±è´¥ {old_dir}: {e}")
                else:
                    print(f"  ä¿ç•™ {old_dir_name} (ä»æœ‰ {len(h5ad_files)} ä¸ªæ•°æ®æ–‡ä»¶)")
    
    def organize_harmony_data(self):
        """æ•´ç†harmonyç›¸å…³æ•°æ®"""
        print("ğŸµ æ•´ç†Harmonyæ•°æ®...")
        
        harmony_dir = self.data_dir / 'harmony'
        if not harmony_dir.exists():
            return
        
        # ç§»åŠ¨Pythonè„šæœ¬åˆ°scriptsç›®å½•
        scripts_dir = self.data_dir / 'scripts' / 'harmony'
        scripts_dir.mkdir(parents=True, exist_ok=True)
        
        for py_file in harmony_dir.glob("*.py"):
            target_path = scripts_dir / py_file.name
            if not target_path.exists():
                try:
                    shutil.move(py_file, target_path)
                    print(f"  ç§»åŠ¨è„šæœ¬: {py_file.name} -> scripts/harmony/")
                except Exception as e:
                    self.errors.append(f"ç§»åŠ¨è„šæœ¬å¤±è´¥ {py_file}: {e}")
        
        # ç§»åŠ¨å›¾ç‰‡åˆ°documentation
        docs_dir = self.data_dir / 'documentation' / 'harmony_figures'
        figures_dir = harmony_dir / 'figures'
        if figures_dir.exists():
            docs_dir.mkdir(parents=True, exist_ok=True)
            try:
                for fig_file in figures_dir.iterdir():
                    target_path = docs_dir / fig_file.name
                    if not target_path.exists():
                        shutil.move(fig_file, target_path)
                print(f"  ç§»åŠ¨å›¾ç‰‡: figures/ -> documentation/harmony_figures/")
            except Exception as e:
                self.errors.append(f"ç§»åŠ¨å›¾ç‰‡å¤±è´¥: {e}")
        
        # ä¿ç•™READMEå’Œé‡è¦çš„h5adæ–‡ä»¶
        for h5ad_file in harmony_dir.rglob("*.h5ad"):
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ¼”ç¤ºæ•°æ®
            if 'demo' in h5ad_file.name.lower() or 'quick' in h5ad_file.name.lower():
                target_dir = self.data_dir / 'demo_datasets'
                target_path = target_dir / h5ad_file.name
                
                if not target_path.exists():
                    try:
                        shutil.move(h5ad_file, target_path)
                        print(f"  ç§»åŠ¨æ•°æ®: {h5ad_file.name} -> demo_datasets/")
                    except Exception as e:
                        self.errors.append(f"ç§»åŠ¨æ•°æ®å¤±è´¥ {h5ad_file}: {e}")
    
    def create_comprehensive_index(self):
        """åˆ›å»ºå…¨é¢çš„æ•°æ®é›†ç´¢å¼•"""
        print("ğŸ“Š åˆ›å»ºç»¼åˆæ•°æ®é›†ç´¢å¼•...")
        
        catalog = {
            'metadata': {
                'created_at': pd.Timestamp.now().isoformat(),
                'total_datasets': 0,
                'total_size_gb': 0,
                'directories': {}
            },
            'datasets_by_category': {},
            'datasets_by_technology': defaultdict(list),
            'datasets_by_size': {
                'small': [],      # < 1000 cells
                'medium': [],     # 1000-10000 cells  
                'large': [],      # 10000-50000 cells
                'xl': []          # > 50000 cells
            },
            'spatial_datasets': [],
            'quick_access': {
                'recommended_spatial': [],
                'recommended_demo': [],
                'recommended_benchmark': []
            }
        }
        
        total_size_bytes = 0
        total_datasets = 0
        
        # æ‰«ææ¯ä¸ªæ ‡å‡†ç›®å½•
        for dir_name in self.standard_dirs.keys():
            dir_path = self.data_dir / dir_name
            if not dir_path.exists():
                continue
            
            datasets = []
            
            for h5ad_file in dir_path.glob("*.h5ad"):
                try:
                    # åˆ†ææ•°æ®é›†
                    adata = sc.read_h5ad(h5ad_file)
                    file_size = h5ad_file.stat().st_size
                    total_size_bytes += file_size
                    total_datasets += 1
                    
                    dataset_info = {
                        'filename': h5ad_file.name,
                        'path': f"{dir_name}/{h5ad_file.name}",
                        'n_cells': adata.n_obs,
                        'n_genes': adata.n_vars,
                        'has_spatial': 'spatial' in adata.obsm,
                        'spatial_dims': adata.obsm['spatial'].shape if 'spatial' in adata.obsm else None,
                        'has_clusters': len(adata.obs.select_dtypes(include=['category', 'object']).columns) > 0,
                        'has_leiden': 'leiden' in adata.obs.columns,
                        'sparsity': float((adata.X == 0).sum() / adata.X.size) if hasattr(adata.X, 'size') else 0.0,
                        'file_size_mb': float(file_size / (1024 * 1024)),
                        'category': dir_name
                    }
                    
                    # æ¨æ–­æŠ€æœ¯ç±»å‹
                    dataset_info['technology'] = self.infer_technology(h5ad_file.name, adata)
                    
                    datasets.append(dataset_info)
                    
                    # åˆ†ç±»åˆ°å¤§å°ç±»åˆ«
                    n_cells = adata.n_obs
                    if n_cells < 1000:
                        catalog['datasets_by_size']['small'].append(dataset_info)
                    elif n_cells < 10000:
                        catalog['datasets_by_size']['medium'].append(dataset_info)
                    elif n_cells < 50000:
                        catalog['datasets_by_size']['large'].append(dataset_info)
                    else:
                        catalog['datasets_by_size']['xl'].append(dataset_info)
                    
                    # æŠ€æœ¯åˆ†ç±»
                    catalog['datasets_by_technology'][dataset_info['technology']].append(dataset_info)
                    
                    # ç©ºé—´æ•°æ®é›†
                    if dataset_info['has_spatial']:
                        catalog['spatial_datasets'].append(dataset_info)
                    
                except Exception as e:
                    self.errors.append(f"åˆ†ææ•°æ®é›†å¤±è´¥ {h5ad_file}: {e}")
                    continue
            
            catalog['datasets_by_category'][dir_name] = datasets
            catalog['metadata']['directories'][dir_name] = {
                'count': len(datasets),
                'description': self.standard_dirs[dir_name]
            }
        
        # æ›´æ–°å…ƒæ•°æ®
        catalog['metadata']['total_datasets'] = total_datasets
        catalog['metadata']['total_size_gb'] = total_size_bytes / (1024**3)
        
        # ç”Ÿæˆæ¨èåˆ—è¡¨
        spatial_sorted = sorted(catalog['spatial_datasets'], key=lambda x: x['n_cells'], reverse=True)
        catalog['quick_access']['recommended_spatial'] = spatial_sorted[:10]
        
        demo_datasets = catalog['datasets_by_category'].get('demo_datasets', [])
        catalog['quick_access']['recommended_demo'] = sorted(demo_datasets, key=lambda x: x['n_cells'])[:5]
        
        benchmark_datasets = catalog['datasets_by_category'].get('benchmark_datasets', [])
        catalog['quick_access']['recommended_benchmark'] = sorted(benchmark_datasets, key=lambda x: x['n_cells'])
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        metadata_dir = self.data_dir / 'metadata'
        metadata_dir.mkdir(exist_ok=True)
        
        with open(metadata_dir / 'comprehensive_catalog.json', 'w') as f:
            json.dump(catalog, f, indent=2)
        
        print(f"åˆ›å»ºäº†åŒ…å« {total_datasets} ä¸ªæ•°æ®é›†çš„ç»¼åˆç´¢å¼•")
        return catalog
    
    def infer_technology(self, filename, adata):
        """æ¨æ–­ç©ºé—´è½¬å½•ç»„æŠ€æœ¯"""
        filename = filename.lower()
        
        if 'visium' in filename:
            return 'visium'
        elif 'slideseq' in filename:
            return 'slide-seq'
        elif 'merfish' in filename:
            return 'merfish'
        elif 'seqfish' in filename:
            return 'seqfish'
        elif 'osmfish' in filename:
            return 'osmfish'
        elif 'st_' in filename or 'spatial' in filename:
            return 'spatial_transcriptomics'
        elif 'synthetic' in filename or 'simulated' in filename:
            return 'synthetic'
        elif 'spatial' in adata.obsm:
            # æ ¹æ®æ•°æ®ç‰¹å¾æ¨æ–­
            n_spots = adata.n_obs
            if n_spots > 30000:
                return 'slide-seq'
            elif n_spots > 2000:
                return 'visium'
            else:
                return 'unknown_spatial'
        else:
            return 'single_cell_rna_seq'
    
    def create_documentation(self, catalog):
        """åˆ›å»ºå®Œæ•´æ–‡æ¡£"""
        print("ğŸ“š åˆ›å»ºæ–‡æ¡£...")
        
        docs_dir = self.data_dir / 'documentation'
        docs_dir.mkdir(exist_ok=True)
        
        # ä¸»ç´¢å¼•æ–‡æ¡£
        self.create_main_index(catalog)
        
        # å¿«é€Ÿå¼€å§‹æŒ‡å—
        self.create_quickstart_guide(catalog)
        
        # APIå‚è€ƒ
        self.create_api_reference(catalog)
        
        # æ›´æ–°å„ç›®å½•çš„README
        self.update_directory_readmes(catalog)
    
    def create_main_index(self, catalog):
        """åˆ›å»ºä¸»ç´¢å¼•æ–‡æ¡£"""
        docs_dir = self.data_dir / 'documentation'
        
        with open(docs_dir / 'MAIN_INDEX.md', 'w') as f:
            f.write(f"# ChatSpatial æ•°æ®é›†å®Œæ•´ç´¢å¼•\n\n")
            f.write(f"**æ›´æ–°æ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ•°æ®é›†æ€»æ•°**: {catalog['metadata']['total_datasets']}\n")
            f.write(f"**æ€»å­˜å‚¨ç©ºé—´**: {catalog['metadata']['total_size_gb']:.1f} GB\n\n")
            
            f.write("## ğŸ“Š æŒ‰ç±»åˆ«ç»Ÿè®¡\n\n")
            for dir_name, info in catalog['metadata']['directories'].items():
                f.write(f"- **{dir_name.replace('_', ' ').title()}**: {info['count']} ä¸ªæ•°æ®é›†\n")
            
            f.write(f"\n## ğŸ”¬ æŒ‰æŠ€æœ¯åˆ†ç±»\n\n")
            for tech, datasets in catalog['datasets_by_technology'].items():
                f.write(f"- **{tech.replace('_', ' ').title()}**: {len(datasets)} ä¸ªæ•°æ®é›†\n")
            
            f.write(f"\n## ğŸ“ æŒ‰è§„æ¨¡åˆ†ç±»\n\n")
            for size_cat, datasets in catalog['datasets_by_size'].items():
                f.write(f"- **{size_cat.title()}**: {len(datasets)} ä¸ªæ•°æ®é›†\n")
            
            f.write(f"\n## ğŸ¯ æ¨èæ•°æ®é›†\n\n")
            
            f.write("### ç©ºé—´åˆ†ææ¨è\n")
            for i, dataset in enumerate(catalog['quick_access']['recommended_spatial'][:5], 1):
                f.write(f"{i}. **{dataset['filename']}** - {dataset['n_cells']:,} cells ({dataset['technology']})\n")
            
            f.write(f"\n### æ¼”ç¤ºæ•™ç¨‹æ¨è\n")
            for i, dataset in enumerate(catalog['quick_access']['recommended_demo'][:3], 1):
                f.write(f"{i}. **{dataset['filename']}** - {dataset['n_cells']:,} cells\n")
            
            f.write(f"\n## ğŸ“– è¯¦ç»†ç›®å½•\n\n")
            for dir_name in catalog['metadata']['directories'].keys():
                f.write(f"- [{dir_name.replace('_', ' ').title()}](../{dir_name}/README.md)\n")
    
    def create_quickstart_guide(self, catalog):
        """åˆ›å»ºå¿«é€Ÿå¼€å§‹æŒ‡å—"""
        docs_dir = self.data_dir / 'documentation'
        
        with open(docs_dir / 'QUICKSTART.md', 'w') as f:
            f.write("# ChatSpatial æ•°æ®é›†å¿«é€Ÿå¼€å§‹\n\n")
            
            f.write("## ğŸš€ ç«‹å³å¼€å§‹\n\n")
            f.write("```python\nimport scanpy as sc\n\n")
            
            # æ¨èå‡ ä¸ªå…¸å‹ä½¿ç”¨æ¡ˆä¾‹
            if catalog['quick_access']['recommended_demo']:
                demo = catalog['quick_access']['recommended_demo'][0]
                f.write(f"# 1. å¿«é€Ÿæ¼”ç¤º\n")
                f.write(f"adata = sc.read_h5ad('data/demo_datasets/{demo['filename']}')\n")
                f.write(f"print(f'Demo data: {{adata.n_obs}} cells, {{adata.n_vars}} genes')\n\n")
            
            if catalog['quick_access']['recommended_spatial']:
                spatial = catalog['quick_access']['recommended_spatial'][0]
                f.write(f"# 2. ç©ºé—´åˆ†æ\n")
                f.write(f"adata = sc.read_h5ad('data/spatial_datasets/{spatial['filename']}')\n")
                f.write(f"print(f'Spatial data: {{adata.n_obs}} spots, {{adata.n_vars}} genes')\n")
                f.write(f"print('Spatial coordinates:', adata.obsm['spatial'].shape)\n\n")
            
            if catalog['datasets_by_size']['large']:
                large = catalog['datasets_by_size']['large'][0]
                f.write(f"# 3. å¤§è§„æ¨¡æ•°æ®\n")
                f.write(f"adata = sc.read_h5ad('data/{large['category']}/{large['filename']}')\n")
                f.write(f"print(f'Large dataset: {{adata.n_obs}} cells')\n")
            
            f.write("```\n\n")
            
            f.write("## ğŸ“‹ æ•°æ®é›†é€‰æ‹©æŒ‡å—\n\n")
            f.write("| ç”¨é€” | æ¨èæ•°æ®é›† | ä½ç½® |\n")
            f.write("|------|------------|------|\n")
            
            for purpose, datasets, location in [
                ('å¿«é€Ÿæµ‹è¯•', catalog['datasets_by_size']['small'][:3], 'demo_datasets/'),
                ('ç©ºé—´åˆ†æ', catalog['quick_access']['recommended_spatial'][:3], 'spatial_datasets/'),
                ('æ€§èƒ½æµ‹è¯•', catalog['datasets_by_size']['large'][:3], 'benchmark_datasets/'),
            ]:
                for dataset in datasets:
                    f.write(f"| {purpose} | {dataset['filename']} | {dataset['category']}/ |\n")
    
    def create_api_reference(self, catalog):
        """åˆ›å»ºAPIå‚è€ƒæ–‡æ¡£"""
        docs_dir = self.data_dir / 'documentation'
        
        with open(docs_dir / 'API_REFERENCE.md', 'w') as f:
            f.write("# ChatSpatial æ•°æ®é›† API å‚è€ƒ\n\n")
            
            f.write("## æ•°æ®åŠ è½½ API\n\n")
            f.write("```python\n")
            f.write("# åŸºæœ¬åŠ è½½\n")
            f.write("import scanpy as sc\n")
            f.write("adata = sc.read_h5ad('data/spatial_datasets/dataset.h5ad')\n\n")
            
            f.write("# æ‰¹é‡åŠ è½½\n")
            f.write("import json\n")
            f.write("with open('data/metadata/comprehensive_catalog.json') as f:\n")
            f.write("    catalog = json.load(f)\n\n")
            
            f.write("# æŒ‰ç±»åˆ«åŠ è½½\n")
            f.write("spatial_datasets = catalog['datasets_by_category']['spatial_datasets']\n")
            f.write("for dataset in spatial_datasets:\n")
            f.write("    adata = sc.read_h5ad(f'data/{dataset[\"path\"]}')\n")
            f.write("```\n\n")
            
            f.write("## æ•°æ®é›†æŸ¥è¯¢ API\n\n")
            f.write("```python\n")
            f.write("# æŸ¥è¯¢ç©ºé—´æ•°æ®é›†\n")
            f.write("spatial_datasets = [d for d in catalog['spatial_datasets'] if d['has_spatial']]\n\n")
            
            f.write("# æŸ¥è¯¢å¤§è§„æ¨¡æ•°æ®é›†\n")
            f.write("large_datasets = [d for d in catalog['datasets_by_size']['large']]\n\n")
            
            f.write("# æŸ¥è¯¢ç‰¹å®šæŠ€æœ¯\n")
            f.write("visium_datasets = catalog['datasets_by_technology']['visium']\n")
            f.write("```\n")
    
    def update_directory_readmes(self, catalog):
        """æ›´æ–°å„ç›®å½•çš„READMEæ–‡ä»¶"""
        for dir_name, datasets in catalog['datasets_by_category'].items():
            if not datasets:
                continue
                
            readme_path = self.data_dir / dir_name / 'README.md'
            
            with open(readme_path, 'w') as f:
                f.write(f"# {dir_name.replace('_', ' ').title()}\n\n")
                f.write(f"{self.standard_dirs.get(dir_name, 'æ•°æ®é›†ç›®å½•')}\n\n")
                
                # ç»Ÿè®¡ä¿¡æ¯
                total_cells = sum(d['n_cells'] for d in datasets)
                total_genes = sum(d['n_genes'] for d in datasets)  
                total_size = sum(d['file_size_mb'] for d in datasets)
                spatial_count = sum(1 for d in datasets if d['has_spatial'])
                
                f.write(f"## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n\n")
                f.write(f"- **æ•°æ®é›†æ•°é‡**: {len(datasets)}\n")
                f.write(f"- **æ€»ç»†èƒæ•°**: {total_cells:,}\n")
                f.write(f"- **æ€»åŸºå› æ•°**: {total_genes:,}\n")
                f.write(f"- **æ€»æ–‡ä»¶å¤§å°**: {total_size:.1f} MB\n")
                f.write(f"- **ç©ºé—´æ•°æ®é›†**: {spatial_count}/{len(datasets)}\n\n")
                
                # æ•°æ®é›†åˆ—è¡¨
                f.write(f"## ğŸ“‹ æ•°æ®é›†åˆ—è¡¨\n\n")
                f.write("| æ•°æ®é›† | ç»†èƒæ•° | åŸºå› æ•° | å¤§å°(MB) | æŠ€æœ¯ | ç©ºé—´ | èšç±» |\n")
                f.write("|--------|--------|--------|----------|------|------|------|\n")
                
                for dataset in sorted(datasets, key=lambda x: x['n_cells'], reverse=True):
                    spatial_icon = "âœ…" if dataset['has_spatial'] else "âŒ"
                    cluster_icon = "âœ…" if dataset['has_leiden'] else "âŒ"
                    
                    f.write(f"| {dataset['filename']} | {dataset['n_cells']:,} | {dataset['n_genes']:,} | {dataset['file_size_mb']:.1f} | {dataset['technology']} | {spatial_icon} | {cluster_icon} |\n")
    
    def run_deep_cleanup(self):
        """è¿è¡Œæ·±åº¦æ¸…ç†"""
        print("ğŸ§¹ å¼€å§‹æ·±åº¦æ¸…ç† ChatSpatial data ç›®å½•")
        print("=" * 60)
        
        try:
            # 1. æŸ¥æ‰¾å’Œå¤„ç†é‡å¤æ–‡ä»¶
            self.find_duplicate_files()
            self.handle_duplicates()
            
            # 2. é‡æ–°ç»„ç»‡æ•°æ®é›†åˆ†ç±»
            self.reorganize_datasets()
            
            # 3. æ•´ç†ç‰¹æ®Šç›®å½•
            self.organize_harmony_data()
            
            # 4. æ¸…ç†æ—§ç›®å½•ç»“æ„
            self.clean_old_directories()
            
            # 5. åˆ›å»ºcomprehensiveç´¢å¼•
            catalog = self.create_comprehensive_index()
            
            # 6. åˆ›å»ºå®Œæ•´æ–‡æ¡£
            self.create_documentation(catalog)
            
            # 7. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            self.generate_final_report(catalog)
            
        except Exception as e:
            self.errors.append(f"æ·±åº¦æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")
            raise
    
    def generate_final_report(self, catalog):
        """ç”Ÿæˆæœ€ç»ˆæ¸…ç†æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("æ·±åº¦æ¸…ç†å®ŒæˆæŠ¥å‘Š")
        print("=" * 60)
        
        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"  - æ€»æ•°æ®é›†: {catalog['metadata']['total_datasets']}")
        print(f"  - æ€»å¤§å°: {catalog['metadata']['total_size_gb']:.1f} GB")
        print(f"  - ç©ºé—´æ•°æ®é›†: {len(catalog['spatial_datasets'])}")
        
        print(f"\nğŸ”„ å¤„ç†ç»Ÿè®¡:")
        print(f"  - å‘ç°é‡å¤æ–‡ä»¶ç»„: {len(self.duplicates_found)}")
        print(f"  - ç§»åŠ¨æ–‡ä»¶: {len(self.moved_files)}")
        print(f"  - å½’æ¡£æ–‡ä»¶: {len(self.removed_files)}")
        
        if self.errors:
            print(f"\nâš ï¸  é”™è¯¯ ({len(self.errors)}):")
            for error in self.errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"  - {error}")
        
        print(f"\nğŸ“ æ–°çš„ç›®å½•ç»“æ„:")
        for dir_name, info in catalog['metadata']['directories'].items():
            if info['count'] > 0:
                print(f"  - {dir_name}: {info['count']} ä¸ªæ•°æ®é›†")
        
        print(f"\nğŸ“š æ–‡æ¡£ä½ç½®:")
        print(f"  - ä¸»ç´¢å¼•: data/documentation/MAIN_INDEX.md")
        print(f"  - å¿«é€Ÿå¼€å§‹: data/documentation/QUICKSTART.md")
        print(f"  - APIå‚è€ƒ: data/documentation/API_REFERENCE.md")
        print(f"  - è¯¦ç»†ç›®å½•: data/metadata/comprehensive_catalog.json")


def main():
    """ä¸»å‡½æ•°"""
    data_dir = "/Users/apple/Research/SpatialTrans_MCP/chatspatial/data"
    
    cleaner = DataDirectoryCleaner(data_dir)
    cleaner.run_deep_cleanup()


if __name__ == "__main__":
    main()