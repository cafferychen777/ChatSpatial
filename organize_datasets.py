#!/usr/bin/env python3
"""
æ•´ç†æ‰€æœ‰æ•°æ®é›†åˆ°ç»Ÿä¸€çš„dataç›®å½•ç»“æ„ä¸­
"""

import os
import shutil
from pathlib import Path
import json
import scanpy as sc
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

def create_data_structure():
    """åˆ›å»ºæ ‡å‡†çš„dataç›®å½•ç»“æ„"""
    base_dir = Path("/Users/apple/Research/SpatialTrans_MCP/chatspatial/data")
    
    # åˆ›å»ºæ ‡å‡†ç›®å½•ç»“æ„
    directories = {
        'real_datasets': 'çœŸå®çš„ç©ºé—´è½¬å½•ç»„æ•°æ®é›†',
        'synthetic_datasets': 'åˆæˆ/æ¨¡æ‹Ÿæ•°æ®é›†',
        'benchmark_datasets': 'æ€§èƒ½åŸºå‡†æµ‹è¯•æ•°æ®é›†',
        'demo_datasets': 'æ¼”ç¤ºå’Œæ•™ç¨‹æ•°æ®é›†',
        'reference_datasets': 'å‚è€ƒå’Œæ ‡å‡†æ•°æ®é›†',
        'metadata': 'æ•°æ®é›†å…ƒä¿¡æ¯å’Œç´¢å¼•'
    }
    
    for dir_name, description in directories.items():
        dir_path = base_dir / dir_name
        dir_path.mkdir(exist_ok=True)
        
        # åˆ›å»ºREADMEæ–‡ä»¶
        readme_path = dir_path / 'README.md'
        if not readme_path.exists():
            with open(readme_path, 'w') as f:
                f.write(f"# {dir_name.replace('_', ' ').title()}\n\n")
                f.write(f"{description}\n\n")
                f.write("## æ•°æ®é›†åˆ—è¡¨\n\n")
                f.write("æ›´æ–°æ—¶é—´: å¾…æ›´æ–°\n")
    
    return base_dir, directories

def collect_all_datasets():
    """æ”¶é›†æ‰€æœ‰ç°æœ‰çš„æ•°æ®é›†æ–‡ä»¶"""
    datasets = []
    
    # æœç´¢è·¯å¾„
    search_paths = [
        Path("/Users/apple/Research/SpatialTrans_MCP/chatspatial/data"),
        Path("/Users/apple/Research/SpatialTrans_MCP/chatspatial/tests/comprehensive_testing_2024"),
    ]
    
    for search_path in search_paths:
        if search_path.exists():
            # é€’å½’æŸ¥æ‰¾æ‰€æœ‰h5adæ–‡ä»¶
            for h5ad_file in search_path.rglob("*.h5ad"):
                if h5ad_file.is_file():
                    datasets.append({
                        'current_path': h5ad_file,
                        'filename': h5ad_file.name,
                        'size_mb': h5ad_file.stat().st_size / (1024 * 1024),
                        'source_dir': h5ad_file.parent.name
                    })
    
    print(f"æ‰¾åˆ° {len(datasets)} ä¸ªæ•°æ®é›†æ–‡ä»¶")
    return datasets

def categorize_dataset(dataset_info):
    """æ ¹æ®æ–‡ä»¶åå’Œè·¯å¾„åˆ¤æ–­æ•°æ®é›†ç±»åˆ«"""
    filename = dataset_info['filename'].lower()
    source_dir = dataset_info['source_dir'].lower()
    
    # åˆ†ç±»è§„åˆ™
    if any(x in filename for x in ['synthetic', 'simulated', 'fake', 'artificial']):
        return 'synthetic_datasets'
    elif any(x in filename for x in ['benchmark', 'perf_test', 'stress']):
        return 'benchmark_datasets'  
    elif any(x in filename for x in ['demo', 'tutorial', 'example', 'quick']):
        return 'demo_datasets'
    elif any(x in filename for x in ['paul15', 'pancreas_tiny', 'pancreas_subset']):
        return 'reference_datasets'
    elif any(x in filename for x in ['squidpy', 'slideseq', 'visium', 'merfish', 'seqfish', 'osmfish', 'starmap']):
        return 'real_datasets'
    elif 'harmony' in source_dir:
        return 'demo_datasets'  # harmonyæ•°æ®ä¸»è¦ç”¨äºæ¼”ç¤º
    elif 'test' in source_dir or 'benchmark' in source_dir:
        return 'benchmark_datasets'
    else:
        return 'real_datasets'  # é»˜è®¤å½’ç±»ä¸ºçœŸå®æ•°æ®

def analyze_dataset(file_path):
    """åˆ†æå•ä¸ªæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        adata = sc.read_h5ad(file_path)
        
        info = {
            'filename': file_path.name,
            'n_cells': adata.n_obs,
            'n_genes': adata.n_vars,
            'has_spatial': 'spatial' in adata.obsm,
            'spatial_dims': adata.obsm['spatial'].shape if 'spatial' in adata.obsm else None,
            'has_clusters': len(adata.obs.select_dtypes(include=['category', 'object']).columns) > 0,
            'cluster_keys': list(adata.obs.select_dtypes(include=['category', 'object']).columns),
            'has_leiden': 'leiden' in adata.obs.columns,
            'sparsity': float((adata.X == 0).sum() / adata.X.size) if hasattr(adata.X, 'size') else 0.0,
            'file_size_mb': float(file_path.stat().st_size / (1024 * 1024)),
            'memory_usage_mb': float(adata.X.data.nbytes / (1024 * 1024)) if hasattr(adata.X, 'data') else 0.0
        }
        
        # æ£€æµ‹å¯èƒ½çš„æŠ€æœ¯ç±»å‹
        if 'spatial' in adata.obsm:
            n_spots = adata.n_obs
            if n_spots > 20000:
                info['likely_technology'] = 'slide-seq'
            elif n_spots > 2000:
                info['likely_technology'] = 'visium_or_st'  
            elif adata.n_vars < 100:
                info['likely_technology'] = 'targeted_spatial'
            else:
                info['likely_technology'] = 'unknown_spatial'
        else:
            info['likely_technology'] = 'non_spatial'
        
        return info
        
    except Exception as e:
        return {
            'filename': file_path.name,
            'error': str(e),
            'file_size_mb': float(file_path.stat().st_size / (1024 * 1024))
        }

def move_and_organize_datasets(datasets, base_dir):
    """ç§»åŠ¨å’Œç»„ç»‡æ‰€æœ‰æ•°æ®é›†"""
    moved_datasets = {}
    errors = []
    
    for dataset in datasets:
        try:
            # ç¡®å®šç›®æ ‡ç±»åˆ«
            category = categorize_dataset(dataset)
            target_dir = base_dir / category
            target_path = target_dir / dataset['filename']
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ç›®æ ‡ä½ç½®
            if dataset['current_path'].resolve() == target_path.resolve():
                print(f"è·³è¿‡ {dataset['filename']} (å·²åœ¨æ­£ç¡®ä½ç½®)")
                continue
            
            # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if target_path.exists():
                # æ¯”è¾ƒæ–‡ä»¶å¤§å°ï¼Œå†³å®šæ˜¯å¦è¦†ç›–
                current_size = dataset['current_path'].stat().st_size
                existing_size = target_path.stat().st_size
                
                if current_size == existing_size:
                    print(f"è·³è¿‡ {dataset['filename']} (ç›®æ ‡ä½ç½®å·²æœ‰ç›¸åŒæ–‡ä»¶)")
                    continue
                else:
                    # åˆ›å»ºå¤‡ä»½åç§°
                    backup_name = f"{target_path.stem}_backup_{int(existing_size/(1024*1024))}MB{target_path.suffix}"
                    backup_path = target_dir / backup_name
                    shutil.move(target_path, backup_path)
                    print(f"å¤‡ä»½ç°æœ‰æ–‡ä»¶: {backup_name}")
            
            # ç§»åŠ¨æ–‡ä»¶
            shutil.move(dataset['current_path'], target_path)
            print(f"ç§»åŠ¨ {dataset['filename']} -> {category}/")
            
            # åˆ†ææ•°æ®é›†
            dataset_info = analyze_dataset(target_path)
            
            if category not in moved_datasets:
                moved_datasets[category] = []
            moved_datasets[category].append(dataset_info)
            
        except Exception as e:
            error_msg = f"å¤„ç† {dataset['filename']} æ—¶å‡ºé”™: {str(e)}"
            print(f"âŒ {error_msg}")
            errors.append(error_msg)
    
    return moved_datasets, errors

def update_readme_files(base_dir, moved_datasets):
    """æ›´æ–°å„ç›®å½•çš„READMEæ–‡ä»¶"""
    
    for category, datasets in moved_datasets.items():
        readme_path = base_dir / category / 'README.md'
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_datasets = len(datasets)
        total_cells = sum(d.get('n_cells', 0) for d in datasets if 'n_cells' in d)
        total_genes = sum(d.get('n_genes', 0) for d in datasets if 'n_genes' in d) 
        total_size_mb = sum(d.get('file_size_mb', 0) for d in datasets)
        spatial_datasets = sum(1 for d in datasets if d.get('has_spatial', False))
        
        with open(readme_path, 'w') as f:
            f.write(f"# {category.replace('_', ' ').title()}\n\n")
            
            f.write(f"## ç»Ÿè®¡ä¿¡æ¯\n\n")
            f.write(f"- **æ•°æ®é›†æ•°é‡**: {total_datasets}\n")
            f.write(f"- **æ€»ç»†èƒæ•°**: {total_cells:,}\n")
            f.write(f"- **æ€»åŸºå› æ•°**: {total_genes:,}\n") 
            f.write(f"- **æ€»æ–‡ä»¶å¤§å°**: {total_size_mb:.1f} MB\n")
            f.write(f"- **ç©ºé—´æ•°æ®é›†**: {spatial_datasets}/{total_datasets}\n\n")
            
            f.write(f"## æ•°æ®é›†åˆ—è¡¨\n\n")
            f.write("| æ•°æ®é›† | ç»†èƒæ•° | åŸºå› æ•° | å¤§å°(MB) | ç©ºé—´åæ ‡ | æŠ€æœ¯ç±»å‹ | èšç±»ä¿¡æ¯ |\n")
            f.write("|--------|--------|--------|----------|----------|----------|----------|\n")
            
            for dataset in sorted(datasets, key=lambda x: x.get('n_cells', 0), reverse=True):
                if 'error' in dataset:
                    f.write(f"| {dataset['filename']} | - | - | {dataset['file_size_mb']:.1f} | âŒ åŠ è½½å¤±è´¥ | - | - |\n")
                else:
                    spatial_icon = "âœ…" if dataset.get('has_spatial', False) else "âŒ"
                    cluster_info = "âœ…" if dataset.get('has_leiden', False) else f"{len(dataset.get('cluster_keys', []))} keys" if dataset.get('has_clusters', False) else "âŒ"
                    
                    f.write(f"| {dataset['filename']} | {dataset.get('n_cells', 0):,} | {dataset.get('n_genes', 0):,} | {dataset.get('file_size_mb', 0):.1f} | {spatial_icon} | {dataset.get('likely_technology', 'unknown')} | {cluster_info} |\n")
            
            f.write(f"\næ›´æ–°æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

def create_master_catalog(base_dir, moved_datasets):
    """åˆ›å»ºä¸»æ•°æ®é›†ç›®å½•"""
    catalog = {
        'metadata': {
            'created_at': pd.Timestamp.now().isoformat(),
            'total_categories': len(moved_datasets),
            'total_datasets': sum(len(datasets) for datasets in moved_datasets.values()),
            'total_size_mb': sum(d.get('file_size_mb', 0) for datasets in moved_datasets.values() for d in datasets)
        },
        'categories': moved_datasets,
        'quick_access': {
            'spatial_datasets': [],
            'large_datasets': [],
            'demo_datasets': [],
            'benchmark_datasets': []
        }
    }
    
    # å¡«å……å¿«é€Ÿè®¿é—®ç´¢å¼•
    for category, datasets in moved_datasets.items():
        for dataset in datasets:
            if 'error' not in dataset:
                if dataset.get('has_spatial', False):
                    catalog['quick_access']['spatial_datasets'].append({
                        'filename': dataset['filename'],
                        'category': category,
                        'n_cells': dataset.get('n_cells', 0),
                        'technology': dataset.get('likely_technology', 'unknown')
                    })
                
                if dataset.get('n_cells', 0) > 10000:
                    catalog['quick_access']['large_datasets'].append({
                        'filename': dataset['filename'],
                        'category': category,
                        'n_cells': dataset.get('n_cells', 0)
                    })
    
    # æŒ‰ç±»åˆ«æ·»åŠ å¿«é€Ÿè®¿é—®
    if 'demo_datasets' in moved_datasets:
        catalog['quick_access']['demo_datasets'] = [
            {'filename': d['filename'], 'n_cells': d.get('n_cells', 0)} 
            for d in moved_datasets['demo_datasets'] if 'error' not in d
        ]
    
    if 'benchmark_datasets' in moved_datasets:
        catalog['quick_access']['benchmark_datasets'] = [
            {'filename': d['filename'], 'n_cells': d.get('n_cells', 0)}
            for d in moved_datasets['benchmark_datasets'] if 'error' not in d
        ]
    
    # ä¿å­˜ç›®å½•æ–‡ä»¶
    catalog_path = base_dir / 'metadata' / 'datasets_catalog.json'
    with open(catalog_path, 'w') as f:
        json.dump(catalog, f, indent=2)
    
    print(f"ä¸»æ•°æ®é›†ç›®å½•å·²ä¿å­˜: {catalog_path}")
    
    # åˆ›å»ºç®€åŒ–çš„ç´¢å¼•æ–‡ä»¶
    index_path = base_dir / 'DATASETS_INDEX.md'
    with open(index_path, 'w') as f:
        f.write("# ChatSpatial æ•°æ®é›†ç´¢å¼•\n\n")
        f.write(f"**æ›´æ–°æ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ€»æ•°æ®é›†**: {catalog['metadata']['total_datasets']}\n")
        f.write(f"**æ€»å¤§å°**: {catalog['metadata']['total_size_mb']:.1f} MB\n\n")
        
        f.write("## å¿«é€Ÿè®¿é—®\n\n")
        
        f.write("### ğŸ”¬ ç©ºé—´æ•°æ®é›† (æ¨èç”¨äºç©ºé—´åˆ†æ)\n")
        for dataset in sorted(catalog['quick_access']['spatial_datasets'], key=lambda x: x['n_cells'], reverse=True)[:10]:
            f.write(f"- **{dataset['filename']}**: {dataset['n_cells']:,} cells ({dataset['technology']})\n")
        
        f.write("\n### ğŸ“Š å¤§è§„æ¨¡æ•°æ®é›† (æ¨èç”¨äºæ€§èƒ½æµ‹è¯•)\n")
        for dataset in sorted(catalog['quick_access']['large_datasets'], key=lambda x: x['n_cells'], reverse=True)[:5]:
            f.write(f"- **{dataset['filename']}**: {dataset['n_cells']:,} cells\n")
        
        f.write("\n### ğŸ¯ æ¼”ç¤ºæ•°æ®é›† (æ¨èç”¨äºæ•™ç¨‹)\n")
        for dataset in sorted(catalog['quick_access']['demo_datasets'], key=lambda x: x['n_cells'])[:5]:
            f.write(f"- **{dataset['filename']}**: {dataset['n_cells']:,} cells\n")
        
        f.write(f"\n## è¯¦ç»†åˆ†ç±»\n\n")
        for category in moved_datasets.keys():
            f.write(f"- **[{category.replace('_', ' ').title()}]({category}/README.md)**: {len(moved_datasets[category])} datasets\n")
    
    print(f"æ•°æ®é›†ç´¢å¼•å·²åˆ›å»º: {index_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æ•´ç†ChatSpatialæ•°æ®é›†...")
    print("="*50)
    
    # 1. åˆ›å»ºç›®å½•ç»“æ„
    print("1. åˆ›å»ºæ ‡å‡†ç›®å½•ç»“æ„...")
    base_dir, directories = create_data_structure()
    
    # 2. æ”¶é›†æ‰€æœ‰æ•°æ®é›†
    print("\n2. æ”¶é›†ç°æœ‰æ•°æ®é›†...")
    datasets = collect_all_datasets()
    
    if not datasets:
        print("æ²¡æœ‰æ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶")
        return
    
    # 3. ç§»åŠ¨å’Œç»„ç»‡æ•°æ®é›†
    print(f"\n3. ç§»åŠ¨å’Œç»„ç»‡ {len(datasets)} ä¸ªæ•°æ®é›†...")
    moved_datasets, errors = move_and_organize_datasets(datasets, base_dir)
    
    # 4. æ›´æ–°READMEæ–‡ä»¶
    print("\n4. æ›´æ–°æ–‡æ¡£...")
    update_readme_files(base_dir, moved_datasets)
    
    # 5. åˆ›å»ºä¸»ç›®å½•
    print("\n5. åˆ›å»ºä¸»æ•°æ®é›†ç›®å½•...")
    create_master_catalog(base_dir, moved_datasets)
    
    # 6. æ¸…ç†ç©ºç›®å½•
    print("\n6. æ¸…ç†æµ‹è¯•ç›®å½•...")
    test_datasets_dir = Path("/Users/apple/Research/SpatialTrans_MCP/chatspatial/tests/comprehensive_testing_2024/datasets")
    if test_datasets_dir.exists():
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰h5adæ–‡ä»¶
        remaining_files = list(test_datasets_dir.rglob("*.h5ad"))
        if not remaining_files:
            print("æµ‹è¯•ç›®å½•ä¸­æ— å‰©ä½™æ•°æ®é›†ï¼Œä¿ç•™ç›®å½•ç»“æ„")
        else:
            print(f"æµ‹è¯•ç›®å½•ä¸­è¿˜æœ‰ {len(remaining_files)} ä¸ªæ–‡ä»¶æœªå¤„ç†")
    
    # 7. æœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*50)
    print("æ•°æ®é›†æ•´ç†å®Œæˆ!")
    print("="*50)
    
    total_moved = sum(len(datasets) for datasets in moved_datasets.values())
    print(f"âœ… æˆåŠŸæ•´ç† {total_moved} ä¸ªæ•°æ®é›†")
    
    for category, datasets in moved_datasets.items():
        valid_datasets = [d for d in datasets if 'error' not in d]
        error_datasets = [d for d in datasets if 'error' in d]
        print(f"   ğŸ“ {category}: {len(valid_datasets)} æœ‰æ•ˆ, {len(error_datasets)} é”™è¯¯")
    
    if errors:
        print(f"\nâš ï¸  å¤„ç†è¿‡ç¨‹ä¸­å‡ºç° {len(errors)} ä¸ªé”™è¯¯:")
        for error in errors:
            print(f"   - {error}")
    
    print(f"\nğŸ“ æ•°æ®é›†ä½ç½®: {base_dir}")
    print(f"ğŸ“– æŸ¥çœ‹ç´¢å¼•: {base_dir}/DATASETS_INDEX.md")
    print(f"ğŸ” è¯¦ç»†ç›®å½•: {base_dir}/metadata/datasets_catalog.json")

if __name__ == "__main__":
    main()